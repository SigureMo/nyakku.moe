---
title: Deep Learning
date: 2018-10-14
category: 墨
tags:
  - CS
  - DL
---

::: tip

DeepLearning, Andrew Ng 慕课笔记

:::

<!-- more -->

## 1 Neural Networks and DeepLearning

### 1.1 Introduction to DeepLearning

#### 1.1.1 Welcome

#### 1.1.2 What is a Neural Network

#### 1.1.3 Supervised Learning with Neural Networks

Structured Data ： Database
Unstructured Data : Image Audio Text

#### 1.1.4 Why is Deep Learning taking off?

![DeepLearning01](../img/Deep_Learning/DL01.png)

> 数据量的增加，使得深度学习的优势越来越明显

![DeepLearning02](../img/Deep_Learning/DL02.png)

> 迭代过程需要的时间越长，所能实现的想法就越少，因此训练的算法改良对深度学习来说是一个很关键的问题。
> 比如使用 sigmoid 函数作为激活函数时，过大的输入会使神经网络饱和，梯度下降算法越来越慢；改良为 ReLU 函数（修正线性单元）后，梯度下降的算法运行的更快，迭代时间相应就减少了很多

#### 1.1.5 About this Course

#### 1.1.6 Course Resources

### 1.2 Basics of Neural Network programming

#### 1.2.1 Binary Classification

- 如果想要识别图片中是否有猫，可以将输入和输出组织成这样：输入为 64 \* 64 \* 3 维（像素点 64 \* 64，三个颜色通道），输出为一个数字（0 或者 1）
- 我们对输入和输出符号做以下约定：
  - nx 为输入的维度，比如这里就是 64 \* 64 \* 3
  - 用 X 将 x 按列组织起来
  - 用 Y 将 y 按行组织起来
  - 我们可以用 Python 里的`X.shape()`查看规模

#### 1.2.2 Logistic Regression

1. 相对于输入一个$X$，输出$y$代表这个图片是不是猫，我们更喜欢它能够输出这个图片是猫的概率，这里便用$\hat{y}$表示
2. 我们可以使用$\hat{y} = w^T+b$来表示这个关系，其中$w$是特征权重，维度与特征向量相同，但是这个函数是关于$x$的一个线性函数，这无疑是很糟糕的，因为我们想要的$\hat{y}$是一个在 0 到 1 之间的数呀
3. 这个时候我们就会使用到**sigmoid**函数了，这个神奇的函数将整个实数域映射到了 0-1 之间，将线性映射为非线性，那么，继续吧

#### 1.2.3 Logistic Regression Cost Function

![DeepLearning03](../img/Deep_Learning/DL03.png)

1. 计算误差时，如果使用误差的平方作为损失函数很有可能得到多个“低谷”（很多局部最优），使用梯度下降法将得不到真正最优的解
2. 我们使用$-(ylog\hat{y}+(1-y)log(1-\hat{y}))$作为损失函数，至于为什么可以使 y 分别等于 0 和 1 推算一下（y 也只有这么两个可取的值）
3. 相对于只作用于某一样本的 loss function（损失函数），cost function（成本函数）J 是作用于整个样本集上的函数；对于某一样本，我们使用 loss function 来衡量 $y$ 与$\hat{y}$的差距，相对应的，对于某一样本集，我们使用 cost function 来衡量 W 和 b 的效果

#### 1.2.4 Gradient Descent

1. 因为我们选用了 logistic 的损失函数，所以我们将得到一个最优解而不是像平方误差函数那样的多个局部最优解，因此，我们随意地初始化这么一个 w 和 b，它最终都会随着梯度下降逐步收敛于我们想要的那个解
2. 所谓梯度下降，就是在每一个状态下沿着下降最快的方向降低
3. 现在先不考虑 b，只考虑 w，我们很容易根据 J 对 w 的导数得到使得 J 下降的 w 的方向，所以我们根据这个“方向”对 w 进行调整，就像：$w -= \alpha \frac{dJ(w)}{dw}$，这里的$\alpha$称作学习率（learning rate），就是每次更新的步长
4. 扩展到 w 和 b 的情况，我们每次迭代就需要执行$w -=  \alpha \frac{\partial J(w,b)}{\partial w}$和$b -=  \alpha \frac{\partial J(w,b)}{\partial b}$

#### 1.2.5 Derivatives

#### 1.2.6 More Derivative Examples

#### 1.2.7 Computation Graph

前向传播

#### 1.2.8 Derivatives with a Computation Graph

1. 反向传播
2. 链式法则
3. 导数用 dvar

#### 1.2.9 Logistic Regression Gradient Descent

1. 首先把正向传播的公式列出来：
   1. $z=w^T+b$
   2. $\hat{y}=a=\sigma (z)$
   3. $L(a,y)=-(ylog(a)+(1-y)log(1-a))$
2. 首先计算$da$（a 对 J 的导数，dJ 省略掉），我们很容易求得$\frac{-y}{a}+\frac{1-y}{1-a}$
3. 之后计算$dz$，根据链式法则，我们先计算出$\frac{da}{dz}=a(1-a)$，再乘上刚刚算出来的$da$，我们便可以得到$dz=a-y$，很神奇吧，最终居然这么简单
4. 最后，我们的$dw_1=x_1dz ;dw_2=x_2dz;db=dz$，你没看错，就是这么简单

#### 1.2.10 Gradient Descent on m Examples

1. 现在我们把上面的一个样本的算法应用到整个数据集上，很明显的，我们需要一个 for 循环去遍历 m 个样本，对每个样本进行累加并求取平均值
2. 像前面一样的，我们在每一层循环内先正向传播计算 J，之后反向传播求导计算各项的导数，这并没有什么不同。
3. 值得注意的的是，虽然我们是在计算每一个样本的，但是 w 和 b 是对于整个样本集而言的，所以我们在这里将样本集里所有 w 加和、b 也一样.当然这里只考虑了一个特征的 w，如果有多个特征（n 个），那么我们就对每个 w 都这么做，最终会得到$dw_1$\~$dw_n$还有$db$，诶？这不就是我们要求的吗？那么还剩最后一步了
4. 所谓最后一步，不过是让 w 减去$\alpha dw$、d 减去$\alpha db$，完成梯度下降的任务而已
5. 我们这里有两个 for 循环（m 和 n），我们知道，循环是很慢的，如果可以，我们会尽最大能力降低时间复杂度，以获得更高的效率，我们在前面提到过这对想法的实现来说是很重要的一件事，但是具体要怎么提高速度呢，我们继续看下一节——向量化

#### 1.2.11 Vectorization

1. 前面已经提到，如果我们直接使用 for 循环，那么速度会非常的慢，慢到什么程度呢？一个直观的测试告诉我们，它会比向量化慢上 300 倍！下面，就让我们使用向量化来取代这奇慢的循环吧
2. 让我们来尝试：

```Python
import numpy as np
a = np.array([1, 2, 3, 4]) # 我们先试试怎么初始化一个数组
```

```Python
a = np.random.rand(1000000)
b = np.random.rand(1000000)
c = np.dot(a, b) # 这样就完成了点乘了？？？
```

#### 1.2.12 More Examples of Vectorization

1. 让我们再看几个例子：

   1. `np.exp(v) # 对每个元素求exp`
   2. `np.log(v) # 对每个元素求log`
   3. `np.abs(v) # 对每个元素求abs`
   4. `np.maximum(v) # 求整个矩阵的最大值`
   5. `v**2 # 对每个元素求平方`
   6. `1/v # 对每个元素求倒数`

2. 现在，我们试着去优化那个有着两个 for 循环的神经网络过程，我们先简单的优化掉 n 这个 for 循环：

```Python
dw = np.zeros((n-x, 1)) # 因为要用矩阵，就将原来对每个w初始化为0改成初始化这样的一个全零矩阵
# ...
for i in range(m):
    # ...
    dw += xi dzi
    db += dzi
dw /= m
```

很明显，我们已经把里面的 n 循环优化掉了，也就是，可以一次将所有特征看做整体 w，而不必去对每个特征分别计算

3. 很轻松地，我们完成了对一个 for 循环的优化，接下来，让我们把所有的 for 循环都优化掉吧 φ(\>ω\<\*)

#### 1.2.13 Vectorizing Logistic Regression

1. 在前面我们利用 X 将各组训练数据按列组织起来，我们现在再看如何利用矩阵对这些数据进行计算
2. 我们先看一个训练集的时候，我们是使用$z^{(i)} = w^Tx^{(i)}+b$和$a^{(i)}=\sigma(z^{(i)})$对每个训练集的前向传播进行计算的
   1. 这里$w$是(n, 1)的矩阵，$w^T$自然就是(1, n)的矩阵（也就是上节课说的将 n 个输入特征组织起来）
   2. 再看$x^{(i)}$，这也是一组训练集的数据，对应于 n 个输入特征，很明显，它是(n, 1)的矩阵
   3. 权重和输入特征矩阵相乘，最后输出的是一个数字（或者说(1, 1)矩阵），我们用它加上 b，自然就是我们想要的值
   4. sigmoid 函数不必说也知道
3. 我们接下来看如何对 m 个训练集同时处理
   1. $w$和$w^T$自然是不变的，他们不可能随着训练集的数量增加而增加，所以它仍然是(1, n)的矩阵
   2. 继续看 X，不必说也知道它是(n, m)的矩阵了，每一列都是刚刚的一个训练集
   3. 然后我们利用矩阵的乘法使两个矩阵相乘，如果我们还对矩阵乘法有点印象的话，就可以知道结果的 i 行 j 列元素对应的就是第一个矩阵的 i 行与第二个矩阵的 j 列相乘并求和，看到这里想必大家都明白了，前面的每一行（虽然现在只有这么一行）不正对应于一个训练集的所对应的权重$w$嘛，后面的每一列不正对应于一个训练集的输入数据嘛，最后的结果不过是将原来的数据列在了一起，形成最后这样的(1, n)的矩阵，比如说(0, 3)是由特征权重 w 和第四组训练集相乘得来，也就是原来 for 循环的第四组
   4. 然后让我们加上 b 吧，既然每个数据都要加 b，那就做一个(1, n)的全 b 矩阵咯
   5. 然后 sigmoid 又不用说了，还是那么简单
4. 下面我们用 Python 试一下

```Python
Z = np.dot(w.t, X) + b # 咦？居然直接这样就可以了？原来numpy会自动将它扩展为(1, n)的全b矩阵
A = σ(Z) # 对每个z求sigmoid
```

#### 1.2.14 Vectorizing Logistic Regression's Gradient

1. 上节课仅仅展示了对前向传播的简化示例，下面我们对反向传播进行简化
2. 经过上节课的讲解，我们应该是知道了我们怎么用矩阵将一组训练集组织起来以及为何可以这么组织起来，这样，我们应该很容易将单个训练集的$dz=a-y$推到整个训练集的$dZ=A-Y$，他们都是(1, m)的矩阵
3. 下面要推的是 dw 和 db，db 我们可以很容易的写出来$db=\frac{1}{m}np.sum(dZ)$，dw 相应地，我们想得到那么一个(1, n)的矩阵代表待优化的 m 个特征，我们可以由每个训练集的$dw=xdz$推出在整个训练集上的$dw=\frac{1}{m}XdZ^T$，哦，你会发现这会得到一个(n, 1)的矩阵，但是这没关系，反正我们要的是他们的和，也就是`dw = np.sum(np.dot(X, dZ.T)) / m`，嗯，我们不仅将算式推出来了，还将 Python 代码写出来了，完美，let's continue.
4. 下面我们就是真正让 w 和 b 优化了，嗯，减一下就好，别忘了学习率
5. 我们完美地、不显式使用一次循环地完成了一次迭代，哦，是整个训练集的一次迭代哦，如果我们多次迭代呢？我们暂时还没有简单的方法使他避免使用 for 循环，那就先使用 for 循环吧，就那样、简单地、在外面嵌套这么一层，就好了

#### 1.2.15 Broadcasting in Python

1. Python 的 numpy 会对某些不满足加减乘除条件的矩阵进行补全，比如某一维度为 1 的矩阵，这样使得很多操作会变得更加方便，这种特性就叫做广播

#### 1.2.16 A note on python or numpy vectors

1. 广播特性更多的是给我们带来了很大的灵活性，使我们写代码更加方便，但如果对广播特性并不了解的话，很容易出现我们不可预料的 bug
2. 我们看`np.random.randn(5)`，你可能会以为他是一个一维的向量，但是通过 print 它和它的转置可以发现，他不过是一个数组而已我们也可以通过观察它的括号层数来判断它的维度
3. 使用上面说的数组很容易出现不可预料的错误，可能你会在使用的时候以为他是一个向量，所以就……那么如何避免呢？我们尽量总是使用`np.random.randn(5, 1)`或者`np.random.randn(1, 5)`这样的代码，另外使用几个 assert(a.sharp == (5, 1))以保证没有写错，还有我们要敢于使用 a.resharp(1, 5)以保证生成的是你想要形状的矩阵

#### 1.2.17 Quick tour of Jupyter/iPython Notebooks

~~emmmmm，我选择暂时不安装，我还是更喜欢原生的 pyshell（idle）~~

#### 1.2.18 Explanation of logistic regression cost function

1. 我们使用$\hat{y}$来输入 x 时表示 y=1 的概率，那么，我们可以得出这样的一组式子：
   - $p(y|x) = \hat{y}$ if y = 1
   - $p(y|x) = 1-\hat{y}$ if y = 0

哦，p(y|x)是什么鬼，让我们回忆下概率的知识，很容易猜出来这是在 x 的前提下 y 的概率，也就是刚刚说的输入 x 时 y 的概率，那就很明了了

2. 我们将这两个式子写在一起$p(y|x) = \hat{y}^y(1-\hat{y})^{(1-y)}$，我们不需要知道为什么这样写，我们只需要知道这样是对的，仔细看看，对吧？
3. 因为我们的损失函数更多的是关注整体的求导之后的方向性（梯度下降最大的方向），我们使用 log 这个严格递增的函数加在刚推出来的概率上，得到$log(p(y|x))=ylog\hat{y}+(1-y)log(1-\hat{y})$，这个就是我们前面提到的损失函数的相反数$-L(\hat{y},y)$。嗯？为什么带个负号？哦我们分别来看，前面我们的**损失函数是什么呢，是计算预测值与实际输出的一个差**，我们想要这个值尽量的小，我们再看看**这里的这个，去掉 log 可是预测满足实际输出的概率**啊，我们当然希望这个值尽量地大，那么这其中的逻辑与关系便可以解释地清楚了
4. 刚刚我们推的是单样本的损失函数，这次我们求整个样本集的成本函数，**根据最大似然估计法，我们已经出现的这个样本集所对应的概率应当是在该情况下出现概率最大的情况**，我们将所有样本的概率乘在一起，然后一样地取 log，这样的话，你会发现每一项都是刚刚所说的损失函数的相反数，我们把负号拿出去，这里我们要的是它取最大值，就是要去掉负号的那个的最小值，这不正是我们想要的成本函数嘛……然后我们再进行适当的缩放，这并不影响它的结果的我们的成本函数便得到了：$J(w,b)=\frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})$

### 1.3 Shallow neural networks

#### 1.3.1 Neural Network Overview

1. 我们已经学习了逻辑回归，现在我们要学习一下神经网络。至于神经网络，简单地说就是让我们上周学习的神经元排列成一个网络……天啊，一个神经元就要写那么多的代码，还要结个网？？？算了不学了不学了！等等，当然这张网是有规律组织起来的我们先看一下这张网长什么样子：

   ![DeepLearning04](../img/Deep_Learning/DL04.png)

2. emmmmm，原来我们还只有一个圈圈，现在一下子变这么多了，不学了，别拦我(╯\>д\<)╯
3. （偷眼瞧）貌似理解错了，我们原来做的貌似不止这么一个圈圈，我们原来的貌似是去掉了中间这四个圈圈的东西，现在的话，貌似只加上了这么一”层“而已，诶呀，这么说我就放心啦
4. 多了一层会有什么后果呢？事实上我们在每个圈圈的位置都使用 w 和 b 对左侧输入进行变换，然后加上激活函数，最左面到中间的一层圈圈这样，中间一层圈圈到最右面一层（个）圈圈也是这样
5. 那么中间这 4 个圈圈怎么计算呢，我们只做过 n 个输入到 1 个输出的情况呀。我们先只看一个圈圈，它经过 w 和 b 会得到一个值，后面几个圈圈也是这样呀，不过对他们处理的 w 和 b 是不一样的，我们使用下标进行区分，我们已经学过了怎么使用矩阵组织多维数据，那么这里我们可以很方便地将各个 w 组织起来
6. 先讲到这里啦，不然下节课没得讲了(～￣ ▽ ￣)～

#### 1.3.2 Neural Network Representation

1. 现在讲讲层与层之间的关系，我们学过了相邻两层之间的关系，也就是说知道了 0-1 和 1-2 的方法，那么还需要知道更多吗？
2. 哦，我只想说没了，有的话，就是记得上标（带中括号的小数字）代表层数，输入层一般不算作一层，或者叫第 0 层$^{[0]}$
3. 现在我们梳理一下各层的维度吧，以便更加直观地理解各层之间的关系：

- $x$($a^{[0]}$) | $(n^{[0]}, 1)$ eg. (3, 1)
  - $W^{[1]}$ | $(n^{[1]}, n^{[0]})$ eg. (4, 3)
  - $b^{[1]}$ | $(n^{[1]}, 1)$ eg. (4, 1)
- $a^{[1]}$ | $(n^{[1]}, 1)$ eg. (4, 1)
  - $W^{[2]}$ | $(1, n^{[1]})$ eg. (1, 4)
  - $b^{[2]}$ | $(1, 1)$ eg. (1, 1)
- $\hat{y}$($a^{[2]}$) | $(1, 1)$ eg. (1, 1)

<!--

| $x$($a^{[0]}$) | $W^{[1]}$            | $a^{[1]}$      | $W^{[2]}$      | $\hat{y}$($a^{[2]}$) |
| -------------- | -------------------- | -------------- | -------------- | -------------------- |
| $(n^{[0]}, 1)$ | $(n^{[1]}, n^{[0]})$ | $(n^{[1]}, 1)$ | $(1, n^{[1]})$ | $(1, 1)$             |
| (3, 1)         | (4, 3)               | (4, 1)         | (1, 4)         | (1, 1)               |
|                | $b^{[1]}$            |                | $b^{[2]}$      |                      |
|                | $(n^{[1]}, 1)$       |                | $(1, 1)$       |                      |
|                | (4, 1)               |                | (1, 1)         |                      |

-->

大概的看一下，我们可以发现，现在满足$a^{[i]}=\sigma(Wa^{[i-1]}+b^{[i]})$，而原来每个式子应该是满足$a^{[i]}=\sigma(w^Ta^{[i-1]}+b^{[i]})$，貌似差了一个转置，为啥呢？下节就可以知道了

#### 1.3.3 Computing a Neural Network's output

1. 嗯，我们先将我们以前的式子列出来$z^{[i]}=w^Ta^{[i-1]}+b^{[i]}$，不要管 i，现在只看某两层之间的关系，那么我们可以通过加下标的方式用以区分各个结点，我们很容易写出来这样的$n^{[i]}$个式子，我们将它按行组织起来，就形成了$W^{[i]}$了呀，哦上节里面的转置便是差在了这里，W 本来就是将 w 的转置组织起来的，自然不需要再转置。
2. 很明显，我们已经在上节把需要推的推出来了，不过当前仅仅针对一次训练数据，我们再看看如何组织数据使得一次可以支持整个训练集的训练

#### 1.3.4 Vectorizing across multiple examples

1. 和之前一样的，我们用列组织训练集的数据，每一列是一次训练数据，我们在上标上加一个带括号的数字来表示这是第几次训练，比如$^{(3)}$代表第三次训练
2. 需要我们组织的有哪些数据呢，首先就是输入的 x，它会被组织成 X，相应地，输出项 y 会被组织成 Y，哦，我们还有考虑中间项，比如 z、a，事实上我们只有 z 和 a，x 和 y 均可以用 a 来表示
3. 我们再看看刚刚我们组织的 A 和 Z，他们各个位置上分别代表什么呢？
   - 从左到右是按列组织的训练集各项，他们用右上角$^{(i)}$来区分
   - 从上到下是各个特征的输入或输出，相关于特征个数 n，他们用右上角$^{[i]}$来区分

#### 1.3.5 Justification for vectorized implementation

1. 前面我们已经在每一节课都认真地分析了总的训练集数据如何组织与为何这样组织，所以这节课可以说是非常轻松了
2. 我们将$w^T$按行组织成一个矩阵，$x$等训练数据按列组织成一个矩阵，至于为什么，我们应该已经很清楚了，$W$中的一行对应于一个输出结点，与$X$中的一列相乘自然就得到了一个输出结点的一次输出数据，我们将此行和此列横向平移和纵向平移，那么我们得到的数据自然就组成了一个矩阵，这个矩阵各个位置分别是啥，前面已经说过
3. 最后我们再通过 Python 的广播机制加一个 b，就完成了

#### 1.3.6 Activation functions

1. 我们之前只用过 sigmoid 函数，但是事实上 sigmoid 函数很少使用的，相对来说 tanh 函数几乎总是比它表现的好，虽然 tanh 只是 sigmoid 向下平移变换就可以得来，但是 tanh 平均值是 0 而不是 0.5，这使得数据可以直接中心化
2. 但是有一个例外情况，就是二分类，它最终需要一个 0~1 的值，而 sigmoid 函数本身就是 0~1 的值，所以我们可以用它作为输出层激活函数，而隐藏层往往使用别的激活函数
3. 另外我们之前有介绍过 ReLu 函数，这是一个很好的激活函数，我们更多情况都是使用这个激活函数
4. 除 ReLu 函数还有一个 Leaky ReLu 函数，不过它并不是特别的常用

#### 1.3.7 why need a nonlinear activation function?

为什么需要这么一个非线性的激活函数呢？课上已经说得很明白了，如果使用线性激活函数那么多层神经网络都可以看成一层神经网络了，所以我们不会在隐层上使用线性激活函数，不过输出层的话是可以考虑的

#### 1.3.8 Derivatives of activation functions

要用梯度下降法算反向传播的话，就一定要计算激活函数的导数咯

1. sigmoid：$a(1-a)$
2. tanh：$1-a^2$
3. ReLU：就是简单的分段函数，因为 0 处不可微，所以可以归到大于 0 或者小于 0 里面

#### 1.3.9 Gradient descent for neural networks

propagation：

1. $Z^{[1]}=W^{[1]}x+b^{[1]}$
2. $A^{[1]}=\sigma(Z^{[1]})$
3. $Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$
4. $A^{[2]}=\sigma(Z^{[2]})$

back propagation：

1. $dZ^{[2]}=A^{[2]}-Y$
2. $dW^{[2]}=\frac{1}{m}dZ^{[2]}A^{[1]T}$
3. $db^{[2]}=\frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True)$
4. $dZ^{[1]}=W^{[2]T}dZ^{[2]}*g^{[1]'}(Z^{[1]})$ # 这里是逐元素相乘
5. $dW^{[1]}=\frac{1}{m}dZ^{[1]}X^{T}$
6. $db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]},axis=1,keepdims=True)$

这里先把我们需要的公式都列出来了，至于为什么，下节课再说（事实上我们每节课都有思索过这些问题，大致上已经知道了），有几点注意的如下：

1. axis=1 表示水平相加求和，db 自然是使用水平相加将所有样本数据加和
2. keepdims 是防止 Python 输出那些古怪的秩数为 1 的矩阵
3. 为了防止 Python 输出古怪的矩阵，可以显示调用 reshape

#### 1.3.10 Backpropagation intuition

没什么，有几个值得注意的地方

1. 反向传播公式 2 和 5 使用的是 X 的转置，那是因为我们当初一层时候使用的是$dw=\frac{1}{m}Xdz^T$现在的 W 是 dw 的转置组织起来的，自然要转置咯
2. 反向传播的时候我们可以通过增加对维度的判断（assert 等），以消除 bug
3. 输出层的梯度是用梯度下降法计算得来的，之后的每层都是根据 W 分配得来

#### 1.3.11 Random+Initialization

1. 如果使用全零初始化权重，那么每次反向传播的值将会均匀的分配到每个隐藏单元对应的权重上，再多的隐藏单元也只相当于一个隐藏单元
2. 我们可以使用随机初始化的权重以保证不会出现上面的情况，另外 b 是可以初始化为 0 的，因为梯度的分配与 W 有关，随机的 W 便可使 b 每次反向传播得到的不相同
3. 我们可以这么初始化权重：`w = np.random.rand(2, 2) * 0.01`，咦，为什么有这么个 0.01，哦，原来是防止太大的初始值使得像 tanh 和 sigmoid 激活函数饱和，梯度太小

### 1.4 Deep Neural Networks

#### 1.4.1 Deep L-layer neural network

1. 我们用 $l$ 表示层数索引
2. 其他的前面提到过，没啥可说的（我发现 1.3.2 那次整理真的很有必要啊，整理之后后面基本都是浏览而过了）

#### 1.4.2 Forward and backward propagation

我们很容易得到前向传播和反向传播的关系式，或者说，我们已经在 1.3.9 写的很明白了这里 copy 一下吧：
propagation：

1. $Z^{[l]}=W^{[l]}·A^{[l-1]}+b^{[1]}$
2. $A^{[1]}=g^{[l]}(Z^{[1]})$

back propagation：

1. $dZ^{[l]}=W^{[l]T}dZ^{[l]}*g^{[l]'}(Z^{[l]})$ # 这里是逐元素相乘
2. $dW^{[l]}=\frac{1}{m}dZ^{[l]}A^{[l-1]T}$
3. $db^{[l]}=\frac{1}{m}np.sum(dZ^{[l]},axis=1,keepdims=True)$
4. $dA^{[l-1]}=W^{[l]T}dZ^{[l]}$

- 前向传播的时候可以将 Z 和 A 缓存起来以便后向传播使用

上周作业已经将这个实现……（因为参考链接根本不告诉我作业题里哪些是要自己写的，哪些是直接给出的，我又最讨厌 copy 源代码，只好边看边加功能）

#### 1.4.3 Forward propagation in a Deep Network

emmmm 貌似没啥，到时候用 for 循环组织各层就好

#### 1.4.4 Getting your matrix dimensions right

维度：

1. $W:(n^{[l]}, n^{[l-1]})$
2. $b:(n^{[l]}, 1)$ # Python 广播可以自动扩展
3. $Z A:(n^{[l]}, m)$

#### 1.4.5 Why deep representations?

1. 深度神经网络是一个从具体到抽象的过程
2. 用人脸识别举例：比如说我们第零层是输入的特征，经过一层 W 的变换后自然得到了一些基本的线这样微小的，而且线性的特征，然后经过一层激活函数，继续传递到下一层，很明显，这一层可以得到上一层的不同组合了，可能我们得到了曲线，当层数不断提高，它所描述的特征越来越复杂，可能某一层已经足够描述某一器官比如鼻子的形状，某一层已经能表征出人脸的大概形状……
3. 用声音识别举例：还是一样，最开始可能只能识别声调高低，之后某层可能识别音位，再之后某层可能已经可以识别单词，最后直到完整的句子
4. 另外至于为什么层数多之后可以很有效地解决某些问题，可以参考电路理论，如果实现同样功能的神经网络，隐藏单元的数量可能成指数倍增长，反过来说就是，很多情况下，深层神经网络会比浅层神经网络好的多

#### 1.4.6 Building blocks of deep neural networks

我们从$y$和$\hat{y}$得到 cost 等信息，然后利用 cost function 的导数计算反向传播梯度下降的的值，也就是$dA^{[l]}$，之后，根据前面 1.4.2 的公式就好，另外$dA^{[0]}$算不算都可以，是个没用的值

#### 1.4.7 Parameters vs Hyperparameters

1. 超参数就是那些控制了我们最终需要的 W、b 的参数，比如学习率、梯度下降法循环的数量、隐藏层的数目、隐藏层单元数目、激活函数的选择，之后我们还会遇到更多
2. 如何选择呢？我们要不断尝试与观察，看最后得到的 J 是否有下降，如此迭代以找到最合适的超参数
3. 另外一个很重要的就是直觉，这当然是要反复尝试锻炼的啦

#### 1.4.8 What does this have to do with the brain?

关系？也许当初是模拟神经元所做的吧，但是神经元绝对比这复杂多了，也许这样真的可以模拟出来大脑，也许根本不一样。总之，深度学习可以达到大脑的效果绝对是夸大的说法，至少，现在是这样的

## 2 Improving Deep Neural Networks

### 2.1 Practical aspects of Deep Learning

#### 2.1.1 Train / Dev / Test sets

1. 超参数的优化是一个反复迭代的过程，即便一个领域的人才，对这个领域有着丰富的经验，到了另外一个领域仍然需要这个过程
2. 事实上，这个优化过程更取决于数据集、计算机配置等因素
3. 我们大多数情况下将数据分为三部分
   - 训练集（training set） 用于训练
   - 验证集（dev set） 根据训练集得到的模型在验证集上的表现调优超参数，直至在该验证集验证下误差达到最小
   - 测试集（test set） 用于评估最终确定的模型的精确度（泛化能力）

::: tip

为什么要分成这么三份？我们来考虑下以下几种情况：

1. 一整个 training set ，很明显不可取，因为这样的话可能最终模型只拟合了 training set 这些数据
2. training set + test set 由于超参数是用 test set 拟合的，所以很可能会出现对 test set 过拟合的现象
3. 而再用一组在训练过程中从未“触碰”过的数据来验证就可以更好地证明模型的正确性

另外，可以对 training set 进行分组，每次取一部分作为 dev set ，这叫做交叉验证，尽管这是一个很好的方法，但是由于训练往往是很耗时的，所以这种方法并不常用

还有一点一定要注意的是，收集数据后尽量将它打乱，如果说将最开始收集到的数据作为 training set ，后收集到的数据作为 test set 的话，很明显这并不满足同分布

:::

4. 小数据量的情况下需要分配很大比例的数据用于测试，但是在当今的大数据时代，这样的做法属实是没太大的必要了，我们完全可以拿出更小的部分用以验证
5. 训练集和验证集要确保来自同一分布
6. 测试集并不是必要的，因为它只是对最终所选定的神经网络做出无偏评估

#### 2.1.2 Bias /Variance

1. 偏差？方差？有区别吗？如果就数学概念而言的话，感觉这两个提不到一块去，不过也确实，他们一个评估的是预测模型，一个评估的是样本数据，但是在 ML 这里我们会经常见到他们的
2. 让我们看下偏差方差与我们训练模型有着什么样的关系：

   ![DeepLearning05](../img/Deep_Learning/DL05.png)

3. 我们再用举一个简单的例子，这次就和实际操作有点联系了：

| Train Set Error | Dev Set Error |               |
| --------------- | ------------- | ------------- |
| 1%              | 11%           | 高偏差        |
| 15%             | 16%           | 高偏差        |
| 15%             | 30%           | 高偏差+高方差 |
| 0.5%            | 1%            | 低偏差+低方差 |

4. 不过偏差与方差也是相对的，前面的假设是基于我们判断的误差接近于 0%的前提下而言的，这里称为**最优误差**或者**贝叶斯误差**。但如果，贝叶斯误差比较高，比如说 15%，此时再看上面的第二个模型，可以说方差与偏差都不高，算是比较合理的了

#### 2.1.3 Basic Recipe for Machine Learning

那么有误差和方差的时候怎么调整呢？

1. 先看**偏差**高不？高的话试试选择**新的网络**，比如增加更多**隐藏层或者隐藏单元**的网络，或者花费**更多时间**来训练网络
2. 当偏差降低到可以接受的数值时，我们利用验证集检查一下**方差**是否有问题，如果方差较高，最好的解决方案就是采用**更多数据**，当然这有点难度，因此我们也可以通过尝试**正则化**来减少过拟合，如果可以找到合适的神经网络框架，有时是可以一箭双雕，同时减少方差和误差的，不过这个过程没有什么捷径了，就是反复的重复尝试……

#### 2.1.4 Regularization

好难 QAQ

1. 想解决**过拟合**问题嘛，大概就需要准备**更多数据**或者**正则化**了，但是我们往往没有获取更多的数据的条件，所以我们可以尝试使用正则化避免过拟合或减少网络误差
2. 我们这里在成本函数 J 上增加一个正则化参数$\lambda$，因为 b 的维度较低，我们完全可以不对它进行正则化
3. 我们通常使用 L2 正则化，它是$\frac{\lambda}{2m}$乘上$w$范数的平方，也就是每个元素平方之和，可以表示为$w^Tw$
4. 而 L1 正则化则是使用$\frac{\lambda}{m}$乘上$\sum_{j=1}^{n_x}|w|$，这样得到的$w$将会是系数的，也就是其中会有很多 0，也许这会节省些内存吧，但是这并不是我们所关注的，所以我们没必要因此去使用它
5. $lambda$是 Python 中的一个保留字，我么可以使用$lambd$来代替它
6. 说这么多，怎么实现呢？我们先写下现在的成本函数：$J(w^{[1]},b^{[1]},...,w^{[2]},b^{[2]})=\frac{1}{m}\sum_{i=1}^{n}L(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{m}\sum_{l=1}^L||W^{[l]}||_F^2$，该矩阵范数被称作“弗罗贝尼乌斯范数”，用下标 F 标注，嗯，先这么叫好了，那么如何实现梯度下降？？？嗯？这个问题应该问吗？求导呗！
7. $dw^{[l]}=backprop+\frac{\lambda}{m}w^{[l]}$，哦，这里的$backprop$是原来没有正则化的$dw^{[l]}$，然后我们就可以进行梯度递降了
8. $w^{[l]} = w^{[l]} - \alpha dw^{[l]} = w^{[l]}-\frac{\alpha\lambda}{m}w^{[l]}-\alpha backprop = (1-\frac{\alpha\lambda}{m})w^{[l]}-\alpha backprop$，这样我们就在$w^{[l]}$前加了一个小于 1 的系数，也就是趋向于使$w^{[l]}$更小，因此，L2 正则化也被称为“权重递减”，但是权重减不减什么的并不是我们所关注的，所以我们不必记住这个名字

::: tip

正则化使得对模型做的任何事情都尽量减轻模型的复杂度，而不是试图去拟合数据

:::

#### 2.1.5 Why regularization reduces overfitting?

1. 我们使用 L2 正则化的时候会有参数$\lambda$，很明显这个参数越大的话，参数 W 会越来越接近于 0，不过这有什么影响呢？我们直观的感受下
2. 这里以激活函数为 tanh 的情况进行举例：

   ![DeepLearning06](../img/Deep_Learning/DL06.png)

   我们知道$z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}$，那么也就是说$W^{[l]}$很小的时候，$z^{[l]}$也会很小，那么再计算$a^{[l]}$时，我们要通过这么一个激活函数对吧，我们可以看到，在激活函数输入很小的时候，它是线性的，而线性的意味着什么呢？再多的隐藏层都可以看做一层，也就是，简化了模型，这就使得模型很可能处于高偏差的状态，然后我们通过减小$\lambda$，就可以调整到那么一个，合适的、Just right 状态

3. 应用梯度下降的时候一定要记得加上正则化项，这样才能保证梯度下降，否则我们可能只能使得原来的梯度下降，并没有使新的梯度函数下降

#### 2.1.6 Dropout Regularization

下面介绍下一种正则化——Dropout（随机失活），这个理解起来相对于 L2 正则化简单得多

1. 我们每次随机地让一部分结点失去作用，比如说以 0.5 的概率，这样会使得我们的神经网络变的更加的简单、规模更小，然后我们用 backprop 方法进行训练
2. 那么如何实现呢？我们最常用的方法是 inverted dropout（反向随机失活），我们使用一个三层（l=3）的网络来举例，我们定义下第三层的 dropout 向量$d^{[3]}$：
   `d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob`
   然后我们看下它时候小于某数，我们称之为 keep-prob，比如 0.5？0.9？它表示保留某个隐藏单元的概率，哦 Python 会把这个变成一个只有 True 和 False 的矩阵，然后怎么用呢？
3. 下面，我们只需要对 a3 进行一点小小的改变

```Python
a3 = np.multiply(a3, d3) # a3 *= d3`
a3 /= keep_prob
```

完了

4. 上面第一步是让结点随机失活，第二步是让期望值保持原来的水平（否则会损失一部分的）
5. 不过我们测试的时候可不能加上 d 的影响， 不然结果会是一个，随机的结果

#### 2.1.7 Understanding Dropout

1. dropout 使得神经网络不依赖于某个输入单元，不会给他过多的权重，因为它随时有可能被清除
2. dropout 可以为每一层设置不同的 keep-prob，我们经常为隐藏单元比较多的层设置较小的 keep-prob，以缓和该层的过拟合现象，而过拟合现象不严重的层可以设置为较高的数值，比如 0.9、1.0，1.0 的话就表示该层将全部保留
3. 不过这也带来一个问题就是，我们要调的超参数更多了
4. 我们通常在计算机视觉上使用 dropout
5. dropout 一大缺点是成本函数 J 不再被明确定义，因为每次都是随机的，我们很难明确的定义 J 使之每次迭代都下降，所以我们可以关闭 dropout，也就是 keep-prob 设为 1，确保 J 单调递减再打开 dropout

#### 2.1.8 Other regularization methods

1. 数据扩增
   再拿猫的识别举例子，当我们数据不够时，我们可以将猫的图片水平翻转或者转动一定角度、放缩等操作，当然我们要保证他还是一只猫，比如竖直翻转什么的就是有点蠢的操作了
2. early stopping
   就是在过拟合之前就、停止继续训练了，但是这个缺点也很明显，就是很可能停下来的位置的 J 还是很高，但为了防止过拟合不得不过早停下来

#### 2.1.9 Normalizing inputs

归一化输入，它包括这么两步：

1. 零均值
   $\mu=\frac{1}{m}\sum_{i=1}^{m}x^{(i)}$
   $x-=\mu$
2. 归一化方差
   $\sigma^2=\frac{1}{m}\sum_{i=1}^mx^{(i)}**2$
   $x/=\sigma^2$

   ![DeepLearning07](../img/Deep_Learning/DL07.png)

- 这样我们就得到了一个，均值为 0，方差为 1 的输入特征，当然，我们要对每个输入特征都进行归一化，这样才能让每一个输入都很……嗯……规范……那么有什么好处呢？？？

- 我们首先看上面第一个“碗”，很明显，这是一个长相很奇怪的代价函数，而且它不是归一化的，如果我们使用梯度下降，那么他将不断地沿着梯度最大（这里体现为“等高线”的法线方向）崎岖的、蜿蜒的路走下去，而且这样我们没办法使用过大的步长（学习率），否则很难到达“终点”
- 我们再看第二个圆整的“碗”，我们直观上会很喜欢这样的一个碗，因为它很标准，每个方向上都是这样，任何一个初始值都会直接奔向“终点”，这使得 J 的优化会加快很多

- 另外，我们是在各特征的范围差距比较大时使用这个是非常必要的，如果各输入特征范围相似度比较高，那么我们使用归一化就不是那么重要了

#### 2.1.10 Vanishing / Exploding gradients

为什么会产生梯度消失和梯度爆炸现象呢？看下面的例子

1. 我们有那么一个**层数很多**的，线性激活的矩阵，我们假设每层的参数 w 都是稍稍小于 1 的对角矩阵，b 当 0 处理，嗯，w 只需要稍稍小于 1，当传递到最后一层会是什么情况呢？不用说也能想象得到，那么我们又如何得到梯度优化参数呢？
2. 就是这样的过程，相反地，如果参数 w 稍稍大于 1，那么最后一层的数据将会是爆炸式的

#### 2.1.11 Weight Initialization for Deep NetworksVanishing / Exploding gradients

1. 如果是一个神经元，我们可能不会希望随着输入层数量的增加而产生过高的输出，所以我们也许会希望除以输入层特征的数量
2. 让我们来计算一下，以免看不懂后面的根号是怎么来的

   1. 然后就算自闭了……
   2. z = wx+~~b~~ = $w_1x_1+w_2x_2+...+w_nx_n$
   3. $a = \sigma(z)$
   4. $D(z)$
      $=D(\sum_{i=1}^nw_ix_i)$
      $=\sum_{i=1}^nD(w_ix_i)$ # 相互独立
      $=nD(w_ix_i)$ # 同分布
   5. $D(w_ix_i)$
      $=E(w_i^2x_i^2)-E^2(w_i)E^2(x_i)$
      $=E(w_i^2)E(x_i^2)-E^2(w_i)E^2(x_i)$

      > 假设：
      > $x$已经均一化 => $E(x_i)=0,D(x_i)=1$ > $w$是以 0 为均值进行的初始化 => $E(w_i)=0$

      $=D(w_i)$

   6. 即$D(z)=nD(w_i)$

      > 我们更期望每层得到的方差不会累积增加，维持在“1”这个 Just right 刚刚好（与$D(x_i)$一样）

      $D(w_i)=\frac{1}{n}$ # 根据$D(aX)=a^2D(X)$很容易得出，$w_i$应每个除$\sqrt{n}$

   7. 完了？？？啊不对，我在算什么啊，下一层可不是 z 哦，下一层是 a 啊，那好说，其实上面的过程是使用线性激活的情况，非线性激活也是一样的步骤啦，下面试着推下 ReLU 作为激活函数的情况
   8. $D(a)$
      $=D\{\sum_{i=1}^nReLU(w_ix_i)\}$
      $=nD\{ReLU(w_ix_i)\}$
   9. $D\{ReLU(w_ix_i)\}$
      $=E\{ReLU^2(w_ix_i)\}-E^2\{ReLU(w_ix_i)\}$
      $=\frac{1}{2}E(w_i^2x_i^2)+\frac{1}{2}E(0)-[\frac{1}{2}E(w_ix_i)+\frac{1}{2}E(0)]^2$
      $=\frac{1}{2}E(w_i^2)E(x_i^2)-\frac{1}{4}E^2(w_ix_i)$
      $=\frac{1}{2}D(w_i)$
   10. $D(a)=\frac{n}{2}D(w_i)$
       $D(w_i)=\frac{2}{n}$
   11. 就是这样，我们就算出来了 ReLU 的随机初始化方差应该取$\frac{2}{n}$，也就是$W^{[l]}=np.random.randn(shape)*np.sqrt(\frac{2}{n^{[l-1]}})$
   12. 当然，这是 ReLU 的情况，它是使用$\sqrt{\frac{2}{n^{[l-1]}}}$，还有其他的激活函数呢，对于 tanh，有人说使用$\sqrt{\frac{1}{n^{[l-1]}}}$，也有人提到使用$\sqrt{\frac{2}{n^{[l-1]}+n^{[l]}}}$
   13. 不过，权重的初始化只是一个起点，这些公式只能给一个好的起点，他们会给出初始化权重矩阵的方差的默认值，如果想增加方差，那么方差会成为另一个超参数，这可以通过在刚才式子上增加一个调优参数实现，但这并不是我们的首要调优参数，所以，我们往往将它的优先级放的比较低。
   14. 边复习边算的，很多东西都忘了，可信度不高(～￣ ▽ ￣)～

#### 2.1.12 Numerical approximation of gradients

使用导数定义近似估计梯度，首先我们看下单边误差

$$
\begin{aligned}
& \lim \limits_{\varepsilon \to 0} \frac{\frac{f(\theta + \varepsilon) - f(\theta)}{\varepsilon} - f'(\theta)}{\varepsilon ^k} \\
=& \lim \limits_{\varepsilon \to 0} \frac{f(\theta + \varepsilon) - f(\theta) - \varepsilon f'(\theta)}{\varepsilon ^{k+1}} \\
=& \lim \limits_{\varepsilon \to 0} \frac{f'(\theta + \varepsilon) - f'(\theta)}{(k+1)\varepsilon ^k} \\
=& \lim \limits_{\varepsilon \to 0} \frac{f''(\theta + \varepsilon)}{k(k+1) \varepsilon ^{k-1}}
\end{aligned}
$$

故 $k-1 = 0$ 即误差是 $\varepsilon$ 的同阶无穷小

下面看看双边误差

$$
\begin{aligned}
& \lim \limits_{\varepsilon \to 0} \frac{\frac{f(\theta + \varepsilon) - f(\theta - \varepsilon)}{2 \varepsilon} - f'(\theta)}{\varepsilon ^k} \\
=& \lim \limits_{\varepsilon \to 0} \frac{f(\theta + \varepsilon) - f(\theta - \varepsilon) - 2 \varepsilon f'(\theta)}{2 \varepsilon ^{k+1}} \\
=& \lim \limits_{\varepsilon \to 0} \frac{f'(\theta + \varepsilon) + f'(\theta - \varepsilon) - 2 f'(\theta)}{2 (k+1)\varepsilon ^k} \\
=& \lim \limits_{\varepsilon \to 0} \frac{f''(\theta + \varepsilon) - f''(\theta - \varepsilon)}{2 k(k+1) \varepsilon ^{k-1}} \\
=& \lim \limits_{\varepsilon \to 0} \frac{f'''(\theta + \varepsilon) + f'''(\theta - \varepsilon)}{2 (k-1)k(k+1) \varepsilon ^{k-2}}
\end{aligned}
$$

故 $k-2 = 0$ 即误差是 $\varepsilon^2$ 的同阶无穷小

也就是说，双边误差比单边误差小，所以我们会用双边误差做梯度检验

#### 2.1.13 Gradient checking

1. 就是利用刚刚所说的双边误差对 backprop 的梯度进行检验

   ![DeepLearning09](../img/Deep_Learning/DL09.png)

2. 按某种规则用$\theta$将所有参数（$w_1\ b_1\ w_2\ b_2\ ...\ w_n\ b_n$）组织起来，嗯，是一个很大的向量，按同样规则用$d\theta$将所有反向传播参数（$dw_1\ db_1\ dw_2\ db_2\ ...\ dw_n\ db_n$）
3. $d\theta_{approx}[i]=\frac{J(\theta_1,\theta_2,...\theta_i+\varepsilon...)-J(\theta_1,\theta_2,...\theta_i-\varepsilon...)}{2\varepsilon}$，这里的$\varepsilon$只在$\theta_i$上加，也就是只对$\theta_i$求的偏导（$\frac{\partial J}{\partial \theta_i}$，用双边的导数定义近似求解），相应地，因为$d\theta$也是这样维度的向量，$d\theta[i]$就表示的是我们自己根据反向传播计算的对$\theta_i$的求导（$dw_1\ db_1\ dw_2\ db_2\ ...\ dw_n\ db_n$都是一步步按反向传播计算出来的，虽然现在按照某种变换改变了，但他们都是对 J 的导数是没有问题的）
4. 我们要求$d\theta_{approx}$和$d\theta$，但我们现在只有各个偏导组成的向量，要怎么度量两个向量是否彼此接近呢？我们可以使用$d\theta_{approx}[i]-d\theta[i]$的欧几里得范数$||d\theta_{approx}-d\theta||_2$，然后对向量的长度进行归一化，也就是$\frac{||d\theta_{approx}-d\theta||_2}{||d\theta_{approx}||2+||d\theta||_2}$，Python 的话，可以使用 np.linalg.norm(grad)
   来求范数
5. 我们看下这个值是否有问题，可以试着将它与$10^{-7}$比较一下，一般比它小的话就说明导数很可能是正确的，但如果高于它，很可能是出 bug 了，去调试吧！

#### 2.1.14 Gradient Checking Implementation Notes

1. 因为梯度检验真的只是对梯度进行检验，没必要在训练的时候也进行，它只是帮我们调试与排除一个 bug 的方法，对训练本身并没有用处，如果真的在训练的时候使用了的话，很可能会花费大量的时间在上面
2. 梯度检验也可以帮助我们大概地定位 bug 的位置，因为如果某个地方出 bug 了，那附近的$d\theta_{approx}$和$d\theta$相差的会比较大
3. 如果使用正则化，那么在梯度检验的时候不要忘了正则项
4. 如果使用 dropout，需要先把 dropout 关闭，然后再进行梯度检验
5. 有的时候会出现这样的情况：$w$和$b$比较小时，梯度下降是正确的，运行梯度下降的时候，$w$和$b$会变得更大，会越来越不准确，为了防止出现这种情况，需要在**随机初始化过程中运行梯度检验，训练网络一段时间后再运行梯度检验**

### 2.2 Optimization algorithms

#### 2.2.1 Mini-batch gradient descent

Mini-batch？好奇怪的名字，但如果我们把以前的方法称作 batch，Mini-batch 就很好理解了。

1. batch 每次是使用整个样本集 m 条数据对网络进行训练，但当 m 比较大的时候呢？这时每次训练完整个样本才能进行一次的梯度下降法，效率就会很低
2. 当样本集数量比较大的时候，我们将样本集拆分为一个个的**mini-batch**，我们每对一个 mini-batch 使用梯度递法，这样会很有效地加快训练的速度
3. 比如说有 5,000,000 个样本数据，我们可以将$x^{(1)}$到$x^{(1000)}$作为第一个 mini-batch，我们把它记作$X^{\{1\}}$，同样地，$x^{(1001)}$到$x^{(2000)}$称为$X^{\{2\}}$and so on.直到$X^{\{5000\}}$，相应的，对 y 也做同样的处理；当然，我们现在需要用一个 for 循环来遍历整个训练集了
4. 我们现在需要如何计算成本？因为自己规模是 1000，$J=\frac{1}{1000}\sum_{i=1}^lL(\hat{y}^{(i)},y^{(i)})$，当然，$L(\hat{y}^{(i)},y^{(i)})$是来自 mini-batch$X^{\{t\}}$和$Y^{\{t\}}$中的样本
5. 如果用到了正则化，可以使用$J=\frac{1}{1000}\sum_{i=1}^lL(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2\cdot1000}\sum_l||\omega^{[l]}||_F^2$

#### 2.2.2 Understanding mini-batch gradient descent

batch 梯度下降时每次迭代成本都会下降（除非学习率过大导致在最优解处不收敛），但如果是 mini-batch，我们更可能看到的是一个整体趋势减小但噪声很大的成本

1. 我们如果选取 1 作为 mini-batch 的话，噪声会非常非常的大
2. 如果选取 m 作为 mini-batch 也就是 m 的话，也就是 batch 梯度下降法，噪声是小了，但是单次迭代耗时太长
3. 我们通常在样本**小于 2000 的时候直接使用 batch 梯度下降法**，**如果大于 2000 的时候设 mini-batch 为 64 到 512**，因为考虑到电脑内存的设置和使用方式，我们最好使用这些 2 的 n 次方型数据
4. 最后需要注意的一点是$X^{\{t\}}$和$Y^{\{t\}}$需要符合 CPU/GPU 内存

#### 2.2.3 Exponentially weighted averages

指数加权平均，也叫指数加权移动平均

1. 首先，我们有这么一堆数据，哦，看起来像一些散点图，那么如何求他们的指数加权平均呢
2.

$$
\begin{aligned}
v_0 & = 0 \\
v_1 & = 0.9v_0+0.1\theta_1 \\
v_1 & = 0.9v_1+0.1\theta_2 \\
v_1 & = 0.9v_2+0.1\theta_3 \\
 & \cdots \\
v_t & = 0.9v_{(t-1)}+0.1\theta_t \\
\end{aligned}
$$

- 这里$\theta_t$表示第 t 天的数据，$v_t$就是指数加权平均数
- 如果我们令这里的 0.9 为$\beta$，那么我们会得到

$$
\begin{aligned}
v_0 & = 0 \\
v_t & = \beta v_{(t-1)} + (1-\beta)\theta_t
\end{aligned}
$$

- 可以看做最近$\frac{1}{1-\beta}$天的平均值

3. $\beta$越大每个数值平均的天数越多，曲线越光滑，$\beta$越小每个数值平均的天数越少，故曲线噪声很大

#### 2.2.4 Understanding exponentially weighted averages

1. 我们依然使用$\beta=0.1$作为例子

   $v_{100} = 0.1\theta_{100} + 0.1\times0.9\theta_{99} + 0.1\times0.9^2\theta_{98} + 0.1\times0.9^3\theta_{97} + 0.1\times0.9^4\theta_{96} + \cdots$

- 将$v_{100}$展开后很容易看出来这是$\theta_t$与一个指数函数相乘（$\beta (1-\beta)^t$，这里以 100 天为原点，向负方向增长）
- 那么为什么是表示最近$\frac{1}{1-\beta}$的平均值呢，我们推一下
  1.  首先，我们知道$\beta$是一个 0-1 的数，它更趋近于 1（越趋近于 0 噪声越大，指数加权平均的意义越小）
  2.  $\lim_{n\rightarrow\infty} (1+\frac{1}{x})^x=e$，emmm，很基本的极限公式
  3.  $\lim_{\beta\rightarrow1^-}\beta^{\frac{1}{1-\beta}}=\lim_{(\beta-1)\rightarrow0^-}[1+(\beta-1)]^{-\frac{1}{(\beta-1)}} = \frac{1}{e}$
  4.  也就是说$\frac{1}{1-\beta}$天后的权重降低到了$(1-\beta)\cdot\frac{1}{e}$之下，可以说是一个足够小的值了

2. 不过，我们如何计算它呢？

```
v = 0
for loop{
    v = beta * v + (1-beta) * theta_t
}
```

3. 由上面的实现方法我们可以知道这种算法很少占用内存，计算的时候只需要读入一个数据并使用计算后的结果覆盖掉原来的数据

#### 2.2.5 Bias correction in exponentially weighted averages

这里讲下对早期数据偏差大的一个优化方案

1. 很明显，令$v_0=0$的话，起点会有点偏低，这导致前期的数据整体处于一个偏低的趋势，当然，后期数据会逐渐弱化掉前期数据的影响，但如果要考虑前期数据的话，我们需要做一下调整
2. 其实也没啥，就是令我们原来求得的$v_t$变成$\frac{v_t}{1-\beta^t}$，那么$v_1=\frac{\beta*v_0+(1-\beta)\theta_1}{1-\beta^1}=\theta_1$，嗯，第一个值已经修正到$\theta_1$了，那么后期数据呢？当$t\rightarrow+\infty$时，修正分母趋近于 1，和原曲线是重合的
3. 注意，是使原来的数据除以$1-\beta^t$进行修正，修正后的数据并不会参与之后的运算，伪码描述大概是这样：

```
v = 0
for loop{
    v = beta * v + (1-beta) * theta_t
}
v /= 1-beta**t // 注意，不是放在forloop内的哦
```

#### 2.2.6 Gradient descent with Momentum

动量梯度下降法？听着很好玩，让我们来看看到底是个什么东西

1. 我们使用梯度递降法的时候经常会遇到这种情况：

   ![DeepLearning10](../img/Deep_Learning/DL10.png)

   每次梯度递降都会不断地波动，指向的方向并不是最终的位置，当然，增大学习率会使得这个现象更加严重

   我们更希望图中竖直方向上的波动更小些，水平方向移动更快些，要如何做呢？

2. 我们试试这个方法：

$$
\begin{aligned}
v_{dW} & = \beta v_{dW} + (1-\beta)dW \\
v_{db} & = \beta v_{db} + (1-\beta)db
\end{aligned}
$$

当然，我们后来梯度递降时候也相应地变成了：

$$
\begin{aligned}
W & = W - \alpha v_{dW} \\
b & = b - \alpha v_{db}
\end{aligned}
$$

- 不过这有什么好处呢？很明显，在梯度递降的过程中，由于移动平均值的相互抵消，我们在纵轴方向上的移动变缓了，又由于$\alpha$是不变的，我们水平方向上必然会变快，所以最终得到的曲线会像红线这样：

  ![DeepLearning11](../img/Deep_Learning/DL11.png)

3. 不过为啥叫动量梯度递降呢？我们可以这样理解：
   - 一个小球从山上向下滚
   - 以$v_{dW} = \beta v_{dW} + (1-\beta)dW$为例
   - $v_{dW}$看作是速度，$dW$看作是加速度，$\beta$看作一个阻力的系数吧，也可以理解成摩擦力什么的，不过这不太严谨
   - 很明显，移动平均后的值使得每次梯度递降的部分受到前几次的影响，类似于速度这样的“连续”的物理量，不像单纯的梯度递降法每次都是独立的
4. 怎么计算呢？我们现在在梯度递降的过程中有了$\alpha$和$\beta$这两个超参数，嗯，计算方法前面已经写出来了，这里再整理下：

$$
\begin{aligned}
v_{dW} & = \beta v_{dW} + (1-\beta)dW \\
v_{db} & = \beta v_{db} + (1-\beta)db \\
W & = W - \alpha v_{dW}, b = b - \alpha v_{db}
\end{aligned}
$$

- 我们经常取$\beta=0.9$
- 至于偏差修正，我们很少使用它，因为那只影响前几次，如果取$\beta=0.9$的话，那只影响前十次的$v_{dW}$，所以并没有太大必要去使用它
- 有时我们会看到去掉$(1-\beta)$的公式，emmm，就是：

$$
\begin{aligned}
v_{dW} & = \beta v_{dW} + dW \\
v_{db} & = \beta v_{db} + db \\
W & = W - \alpha v_{dW}, b = b - \alpha v_{db}
\end{aligned}
$$

当然，它的原理还是 Momentum 法，只不过把$(1-\beta)$隐藏到后面的$\alpha$里了（**后面的推导没太大意义，可以略过**），当然其实$\beta$也是变了的，我们先只看$dW$，把这个式子改写成：

$$
\begin{aligned}
v_{dW} & = \beta' v_{dW} + dW \\
W & = W - \alpha' v_{dW} = W - \alpha'(\beta'v_{dW} + dW)
\end{aligned}
$$

与原来式子相比，我们可以知道：

$$
\alpha'(\beta'v_{dW} + dW) = \alpha(\beta v_{dW} + (1-\beta)dW)
$$

易知：

$$
\begin{aligned}
\alpha & = \alpha'\beta'+\alpha' \\
\beta & = \frac{\alpha'\beta'}{\alpha} = \frac{\alpha'\beta'}{\alpha'\beta'+\alpha'}
\end{aligned}
$$

很明显，原来的$\alpha$与$\beta$是可以用现在的$\alpha$与$\beta$表示出来的，当然，这和原来的式子可以说没有任何区别，但是这个式子表意并不如原来的明确

5. 当然，我们在 batch 或者 mini-batch 都可以使用 Momentum 的方法

#### 2.2.7 RMSprop

和 Momentum 的图一样，我们参考那张图下

1. 我们假设横轴代表$W$，纵轴代表$b$，那么我们使用下面的方法试试：

$$
\begin{aligned}
S_{dW} & = \beta S_{dW} + (1-\beta)dW^2 \\
S_{db} & = \beta S_{db} + (1-\beta)db^2 \\
W & = W - \alpha \frac{dW}{\sqrt{S_{dW}}}, b = b - \alpha \frac{db}{\sqrt{S_{db}}}
\end{aligned}
$$

2. 我们直观上这么理解这个方法：
   - 如果纵轴$b$上移动过快，那么$S_{db}$会是一个比较大的值，$b$梯度下降的梯度下降就会降低，$W$也是一样的，这样做就使得某一项移动不会过快，以消除摆动
   - 当然，这里只是为了简单只考虑了$W$和$b$，事实上$W$也是一个高维度的量，要考虑$W_1, W_2, W_3, \cdots$
   - 为了防止分母为 0，我们经常使用在分母上加一个比较小的值$\varepsilon$（通常取$10^{-8}$）

#### 2.2.8 Adam optimization algorithm

Adam 优化算法，简单的说，这是一个结合了 Momentum 算法和 RMSprop 算法的算法，直接上算法，相信大家也明白

- $Initialize :$
- $v_{dW} = 0, S_{dW} = 0, v_{db} = 0, S_{db} = 0$
- $v_{dW} = \beta_1v_{dW} + (1-\beta_1)dW, v_{db} = \beta_1v_{db} + (1-\beta_1)db$
- $S_{dW} = \beta_2v_{dW} + (1-\beta_2)dW^2, S_{db} = \beta_2v_{db} + (1-\beta_2)db^2$
- $v_{dW}^{corrected} = \frac{v_{dW}}{(1-\beta_1^t)}, v_{db}^{corrected} = \frac{v_{db}}{(1-\beta_1^t)}$
- $S_{dW}^{corrected} = \frac{S_{dW}}{(1-\beta_2^t)}, S_{db}^{corrected} = \frac{S_{db}}{(1-\beta_2^t)}$
- $W = W - \alpha\frac{v_{dW}^{corrected}}{\sqrt{S_{dW}^{corrected}}+\varepsilon}, b = b - \alpha\frac{v_{db}^{corrected}}{\sqrt{S_{db}^{corrected}}+\varepsilon}$

- Adam 通常是使用偏差修正的
- Momentum 和 RMSprop 的参数$\beta$是不一样的，故使用$\beta_1$和$\beta_2$以区分
- 这是一种能够适用于不同神经网络的算法
- 这里这么多超参数，我们如何设置超参数呢？
  1.  $\alpha$，这是要经常调整的
  2.  $\beta_1$，Adam 论文作者推荐 0.9
  3.  $\beta_2$，Adam 论文作者推荐 0.999
  4.  $\varepsilon$，Adam 论文作者推荐$10^{-8}$

#### 2.2.9 Learning rate decay

很明显，我们学习率过大的时候很难收敛在最优解处，这时候如果我们使用递减的学习率，在接近最优解附近学习率已经很小了，就很容易收敛

下面是几种常用的计算方法

1. $\alpha = \frac{1}{1+decayrate*epoch-num}\cdot\alpha_0$
2. $\alpha = 0.95^{epoch-num}\cdot\alpha_0$
3. $\alpha = \frac{k}{\sqrt{epoch-num}}\cdot\alpha_0$
4. 按阶梯状下降的学习率……
5. 手动衰减:joy:

#### 2.2.10 The problem of local optima

1. 在我们的想象中，成本函数 J 和参数 W 之间的关系也许会更像这种形式：

   ![DeepLearning12](../img/Deep_Learning/DL12.png)

   也就是，有着很多个局部最优的解，这些局部最优点是满足什么条件的呢？很明显，在各个方向的偏导都是 0，而且，在该点的凹凸形式也是一样的才行，不过在三维空间上这种情况还算是比较常见的，如果是在高维情况下，这种要求可以说是很苛刻的，如果在某一个点满足了各个方向偏微分是 0，那么我们更多遇到的是这种情况：

   ![DeepLearning13](../img/Deep_Learning/DL13.png)

   也就是一个鞍点，很难碰到一个局部最优，因为在高维情况下这太难得了（$2^n$）

2. 虽然不会长时间困在局部最优的位置，但是，我们很可能长时间处在平缓段，就像下面这样：

   ![DeepLearning14](../img/Deep_Learning/DL14.png)

因为梯度真的很小，所以梯度递降法会使它小心翼翼地沿着“脊”处缓缓移动，直到鞍点处出现微小扰动后才向正确方向走去，这种情况下，Adam 算法可以让它更快地度过平缓段

3. 对高维的实在是难以直观地去理解，不过我们对它们的理解正在不断发展……………………

### 2.3 Hyperparameter tuning

#### 2.3.1 Tuning process

首先，我们先把我们要调整的各个超参数都列出来：

- $\alpha$
- $\beta$
- $\beta1$, $\beta2$, $\varepsilon$
- #$layers$
- #$hidden\ units$
- $learning\ rate\ decay$
- $mini-batch\ size$

我们最优先考虑的一定是$\alpha$，其次呢，就是 Momentum 参数$\beta$了，还有$mini-batch\ size$和#$hidden\ units$，最后考虑#$layers$和$learning\ rate\ decay$

但是我们到底怎么测试超参数怎么取值是比较好的呢？我们可以对考虑的几个超参数进行随机取值，然后在结果比较理想的区域（几个超参数就是几个维度的空间）重新随机取值，就像拿着“放大镜”找东西那样不断地越来越精细地搜索最佳超参数

#### 2.3.2 Using an appropriate scale to pick hyperparameters

虽然说随机取值是个不错的方法，但是这也是要看情况的，我们要根据具体情况选择合适的标尺

- 如果说我们要在 50 到 100 内选#$hidden\ units$，或者在 2 到 4 内选#$layers$，那么很明显，这种随机取值的很合适的
- 我们再看这个：如果我们要在 0.0001 到 1 内选一个$\alpha$，如果我们真的像这样随机选取，那么很明显，有 90%的数据是落在 0.1~1 之间，而 0.0001 到 0.1 只有 10%，这明显不是我们要的，看到这个我们很容易想到 lg()函数，事实上我们确实也是这么做的，这里只需要这么做：
  ```Python
  r = -4 * np.random.rand()
  alpha = 10 ** r
  ```
- 相反地，我们要怎么取$\beta$呢？这个数很明显是一个接近于 1 的值，嗯，很容易想到了，其实就是$1-\alpha$的方式

#### 2.3.3 Pandas VS Caviar（Hyperparameters tuning in practice: Pandas vs. Caviar）

Pandas：没有足够的计算资源，同时只能计算一组超参数

Caviar：计算资源足够，同时计算多组超参数

#### 2.3.4 Normalizing activations in a network

我们之前已经了解到这么一种算法叫做……归一化输入是吧，事实上这确实是一个非常有用的算法，它甚至是可以用在深层网络之中，这个时候我们叫它——Batch 归一化（简称 **BN** ）

1. 我们将归一化$z$作为默认选择，而不是归一化$a$，不过，这也是有争论的
2. 每次归一化只针对某一层，绝对不可能会将某两层数据归一化处理
3. 具体操作呢？

$$
\begin{aligned}
\mu & = \frac{1}{m} \sum_i z^{(i)} \\
\sigma^2 & = \frac{1}{m} \sum_i (z_i - \mu)^2 \\
z_{norm}^{(i)} & = \frac{z^{(i)}-\mu}{\sqrt{\sigma^2 + \varepsilon}}
\end{aligned}
$$

4. 这样，这些$z$就均一化为均值 0 方差 1 咯~
5. 但是，我们也许并不希望$z$的每个分量都是均值 0 方差 1 这样千篇一律的状态，也许隐藏单元需要更加多元化的分布，所以，我们可以使用

$$
\tilde{z}^{(i)} = \gamma z_{norm}^{(i)} + \beta
$$

这里的$\gamma$和$\beta$是模型的学习参数，需要像更新$w$和$b$一样更新它们

6. 你可以随意地设置$\tilde{z}^{(i)}$的均值，如果$\gamma = \sqrt{\sigma^2 + \varepsilon}$而且$\beta = \mu$，那么$\tilde{z}^{(i)} = z^{(i)}$，也就是说，$z^{(i)}$只是某种$\gamma$和$\beta$取值下的$\tilde{z}^{(i)}$
7. 隐藏单元和训练输入归一化的一个区别是，隐藏单元也许并不一定是均值 0 方差 1
8. $\gamma$和$\beta$的作用是使隐藏单元值的均值和方差标准化，即$z^{(i)}$有固定的均值和方差，至于是多少……当然是要训练的呀~

#### 2.3.5 Fitting Batch Norm into a neural network

1. 之前说过，每一个神经网络单元所做的无非计算$z^{[l]}$和$a^{[l]}$两步，而如果使用 Batch 归一化的话，我们要在这两步之间加一步计算$\tilde{z}^{[l]}$，之后再据此计算$a^{[l]}$
2. 所以说，我们现在每层的参数由$w^{[l]}$和$b^{[l]}$变成了$w^{[l]}$、$b^{[l]}$、$\beta^{[l]}$、$\gamma^{[l]}$四个，当然这里的$\beta$不是 Momentum 里的超参数$\beta$
3. 当然你还是可以结合 Adam 或 RMSprop 或 Momentum 方法而不是单纯地使用 BN
4. BN 的过程虽然看起来很复杂，但是我们如果在框架里使用它的话，它只是一行代码而已，就比如说 Tensorflow ，我们只需要使用`tf.nn.batch_normalization`就可以实现它
5. 实践中经常会将 mini-batch 和 BN 一起使用，
6. 之前有提到使用 BN 后会有四个参数$w^{[l]}$、$b^{[l]}$、$\beta^{[l]}$、$\gamma^{[l]}$，但是事实上$b^{[l]}$已经是没有必要的，因为我们的$z^{[l]} = w^{[l]} a{[l-1]} + b^{[l]}$，但是之后的归一化会将所有的$z^{[l]}$变换为均值为 0，方差为 1，再由$\gamma$和$\beta$重新缩放，所以$b^{[l]}$其实是在这个过程中被消除掉了
7. 所以，接下来，我们要做的就是反向 prop 也就是

$$
\begin{aligned}
w^{[l]} & = w^{[l]} - \alpha dw^{[l]} \\
\beta^{[l]} & = \beta^{[l]} - \alpha d\beta^{[l]} \\
\gamma^{[l]} & = \gamma^{[l]} - \alpha d\gamma^{[l]} \\
\end{aligned}
$$

8. 事实上，我们更多的是直接用框架完成这些~

#### 2.3.6 Why does Batch Norm work?

为什么 BN 会有用呐？

1. 第一个原因是，BN 会将各层输入的特征值进行归一化，就和之前的归一化输入是一样的，**每层都会得到“标准”的输入，这对加速学习是非常有用的**
2. 另外，它可以使得网络更加滞后或者说更深层，神经网络的后层比之于前层更加经受得住变化
3. 那么如何经受变化呢？比如说我们一个神经网络适应了某一个分布下的训练集，可以对该分布下训练集做出很好的预测，但是换了一个分布的训练集情况就很有可能会有所不同，这有点像这个神经网络过拟合第一个分布下的训练集，我们称这种情况为 **Covariate shift**
4. 我们随便找到一个深层神经网络，比如说一个 5 层的神经网络，我们遮去前两层暂且不看，那么第三层的输入又是什么样子的呢？当然是没有规律的、分布未知的数据，因为我们根本没有对它的输入进行任何的约束，它只是在前层参数下的一个输出，这样下去，每层的噪声将会非常的大，前层的一个调整将会使后层的分布发生改变，这当然不是我们想看到的
5. 但如果每层的分布是我们可以进行控制的（$\gamma$ 和$\beta$），这样至少**可以保证每层的输入在一个可控的分布下，这使得整个神经网络更加稳定，前层的改变迫使后层的适应程度会减小，也就减弱了层与层之间的联系，使得每层都可以自己学习，稍稍独立于其它层**
6. **另外， BN 还有一个作用就是轻微的正则化**，为什么呢？主要是因为每组 mini-batch 得到的均值和方差都与整个数据集的有一定的噪声，这就使得得到的$\tilde{z}^{[l]}$也是有噪声的，这就使得它有点类似于 dropout 一样，使得后层并不会过分得依赖于前层，当然这只是轻微的正则化，如果想要获得更好的正则化是可以配合 dropout 使用的
7. 当然，正则化并不是 BN 的真正用途，这只是一个意外的结果罢了，也许 ��� 给你的训练带来好处，当然也有可能会带来一些问题

#### 2.3.7 Batch Norm at test time

1. 首先回顾下我们计算 BN 的公式

$$
\begin{aligned}
\mu & = \frac{1}{m} \sum_i z^{(i)} \\
\sigma^2 & = \frac{1}{m} \sum_i (z_i - \mu)^2 \\
z_{norm}^{(i)} & = \frac{z^{(i)}-\mu}{\sqrt{\sigma^2 + \varepsilon}} \\
\tilde{z}^{(i)} & = \gamma z_{norm}^{(i)} + \beta
\end{aligned}
$$

2. 既然要计算每个 mini-batch ，那么$m$就一定是该 mini-batch 的样本数量而不是整个训练集的样本数量，但是我们可以看到，每处理一个数据都需要先计算出$\mu$，再计算出$\sigma$，然后才能对这一个样本进行处理，哦，天呐，算一个样本的$\tilde{z}^{(i)}$要将整个 mini-batch 遍历三次！
3. 为了避免这种遍历，要如何做呢？没错，就是指数加权平均，用它来估算$\mu$和$\sigma$，事实上任何合理的估算方法都是可以的，而在框架里大多都已经有着默认的估算方式，一般都有着比较好的效果

#### 2.3.8 Softmax regression

我们之前所用的都是二分类，比如说输入一张图片分类为是或不是猫，但是很多情况下我们需要做到更多，比如说，输入一张图片分类为猫、狗、鸡或者其他，这要如何做呢？

1. 当然就是 Softmax 啦，之前我们之所以输出的是二分类当然是因为我们在输出层应用的是 Logistic 回归，而且输出层也只设置了一个结点，那么我们只需要在这里进行一些改动，就可以将它变成一个更强大的分类器啦
2. 比如接着刚才那个例子，将图片分为猫、狗、鸡、或者其他，就是说需要 C = 4 种分类，那么我们可以设置这样的四个结点，每个结点输入$z^{[l]}$，经过非线性变换后变成$e^{z^{[l]}}$，临时叫它$t^{[l]}$，因为我们最终要获得每种的概率，所以我们还要将这四个结点的输出进行归一化，也就是都除以他们的和，$a^{[l]} = \frac{t^{[l]}}{\sum_{j=0}^3 t^{[l]}}$
3. 上面的过程就是 Softmax 回归啦

#### 2.3.9 Training a Softmax classifier

事实上在 C = 2 时， Softmax 回归就变成了 logistic 回归，但在 Softmax 中都要怎么做呢？

1. 损失函数，通常使用$L(\hat{y}, y) = -\sum_{j=1}^4 y_j log\hat{y}_j$
2. 反向传播，只需要记住$dz^{[l]} = \hat{y} - y$，剩下的慢慢求偏导就好啦
3. 当然，我们马上就要学习框架了，这使得只需要我们更加注重前向传播的逻辑与设计，而反向传播的求导什么的交给框架来做就好了

#### 2.3.10 Deep Learning frameworks

选择框架的标准：

1. 简单易用，能够将线性代数库做较好的抽象
2. 运行速度，这是我们一直都有提到的问题
3. 开源

一些推荐：

- Caffe / Caffe2
- CNTK
- DL4J
- Keras
- Lasagne
- mxnet
- PaddlePaddle
- TensorFlow
- Theano
- Torch

#### 2.3.11 TensorFlow

1. 首先是一个简单的例子，我们让 TensorFlow 找到使 Cost $(w-5)^2$最小的的参数$w$

```Python
import numpy as np
import tensorflow as tf

# 定义参数 w
w = tf.Variable(0, dtype=tf.float32)
# 定义 Cost function
cost = tf.add(tf.add(w**2, tf.multiply(-10, w)), 25)
# 定义所用训练方法和学习率，这里是使用梯度递降法与0.01的学习率
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)
# 下面几行是惯用的写法
# 首先是创建全局变量
init = tf.global_variables_initializer()
# 之后创建一个TensorFlow session
session = tf.Session()
# 初始化全局变量
session.run(init)
# 下面将会将 w 初始化为0，但是并没有运行，所以 print 出来还是0
print(session.run(w))


# 我们运行一步训练过程（这里是梯度递降法）
session.run(train)
print(session.run(w))

# 我们运行1000次再看看
for i in range(1000):
    session.run(train)
print(session.run(w))
# 应该已经很接近最终答案5了
```

2. 但我们如何像我们之前做的那样将数据喂给它呢？

```Python
import numpy as np
import tensorflow as tf


coefficients = np.array([[1.], [-20.], [25.]])

w = tf.Variable(0, dtype=tf.float32)
# 告诉 tf 数据格式，具体数据之后“喂”
x = tf.placeholder(tf.float32, [3, 1])
# 因为 tf 已经重载了很多的运算符，所以我们完全可以将这个式子写得好看些
# cost = w**2 - 10*w + 25
# 为了方便“喂”，把原来固定参数改成 x
cost = x[0][0]*w**2 + x[1][0]*w +x[2][0]
# 如果不想用梯度递降法而是使用 Adam 优化器等等，改掉这行就可以啦
train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)
init = tf.global_variables_initializer()
session = tf.Session()
session.run(init)
print(session.run(w))

# 在这里慢慢“喂”数据，如果要用 mini-batch ，就用 feed_dict 喂入不同的子集
session.run(train, feed_dict={x:coefficients})
print(session.run(w))

for i in range(1000):
    session.run(train, feed_dict={x:coefficients})
print(session.run(w))
```

3. 也许我们经常遇到 `with` 的形式，这都是一样的，只不过这样的代码更安全

```Python
with tf.Session() as session:
    session.run(init)
    print(session.run(w))
    # And more...
```

4. 就像前面所说的，所有的反向传播求导法则已经在建立前向传播那行（cost=blabla）建立好了，乘啦加啦什么的，都在框架内做好了对应的求导法则，所以我们要做的就是搭搭前向传播，选选方案，改改超参数啦

5. 更多请参见[TensorFlow 官网](https://tensorflow.google.cn/)

## 3 Structuring Machine Learning Projects

### 3.1 ML strategy（1）

#### 3.1.1 Why ML Strategy?

优化的“方向性”问题，如果方向走错了，那么可能是白白浪费了很多的时间

#### 3.1.2 Orthogonalization

为什么说正交呢？正交可以理解为垂直的两根轴，在其中一根轴上前进对另一根轴没有分量，这也就是为什么我们学习中都选择直角坐标系而不是选择其它角度的

1. 我们知道平面内任何不平行的两个矢量都可以将整个平面表示出来，但是，如果是不垂直的两根轴，情况会怎样？
2. 我们想要的是，调好一根轴上前进的长度，然后安心地调另一根轴，不会改变之前调好的数值，但是我们很不幸的发现，如果两个轴不垂直的话，这种情况是很难的，我们好不容易调好一根轴的数值后，再调第二根轴就会把前面调好的数值打乱，也就是我们很难**直观地一次性调好**想要的数值，如果想要调好，就一定要左调调、右调调，不断测试
3. 这才是两根轴，我们经常会遇到更多轴的情况，那可就更复杂了（不敢想象.jpg）
4. 所以，我们要选取互不影响的几根轴，而且知道他们分别在调什么

哦，说了这么多，我们看看我们的这几根轴吧

- 无法很好地拟合 training set
  - 更大的 NN
  - 更好的优化算法，比如 Adam 优化算法
- 无法很好地拟合 dev set
  - 各种正则化
  - 增大 training set
- 无法很好地拟合 test set
  - （对 dev set 过拟合）回退一步，使用更大的 dev set
- 无法很好地在生产环境实践
  - 改变 dev set
  - 改变 cost function
    > 我们很少使用 early stopping ，因为这并不是正交化的一项，它可能会同时影响前两项

#### 3.1.3 Single number evaluation metric

我们如何评估一个算法的优劣呢？也许我们有很多的指标，但是如果我们只用单实数作为指标的话，我们的进展就会快很多

1. 比如说我们有两个分类器 A 、 B ，我们用两个指标来评估他们，一个是 Precision （查准率：在你的分类器标记为猫的例子中，有多少真的是猫）、另一个是 Recall （查全率：对于所有真猫的图片，你的分类器正确识别出了多少百分比）
2. 都说了两个指标很难评估，这种情况下我们很难取舍啊，A B 分类器各有优劣我们该怎么选？
3. 有一个标准的方法是使用$F_1$分数进行评估，P 和 R 的调和平均数：$\frac{2}{\frac{1}{P} + \frac{1}{R}}$
4. 再看另一个例子，比如说我们有一些分类器，在各个地区的评估各有优劣，这种情况下，我们可以尝试使用平均值，这样我们可以快速选出比较好的那个分类器，而不是花很多时间纠结在这上面

#### 3.1.4 Satisficing and optimizing metrics

- 我们看这样的一个例子：

| Classifier | Accuracy | Running time |
| ---------- | -------- | -----------: |
| A          | 90%      |         80ms |
| B          | 92%      |         95ms |
| C          | 95%      |      1,500ms |

这样我们要怎么评估呢？比如说像上面那样弄个单指数，$cost=accuracy-0.5 \times runningTime$ ，emmmmm，不过这样貌似有点过于刻意了

再想一种方法，比如说，我们要求在运行时间不超过 $100ms$ 的前提下寻找准确率最高的分类器，也就是说，对于运行时间只设定一个阈值，达到这个阈值才满足条件，但达到之后，无论再高也不在乎了，所以这里 $B$ 的表现最好

更一般地，我们可以考虑下，如果有 $N$ 个指标的话，我们也许最后只需要留一个指标来做最后的评判，而**其余 $N-1$ 个指标则只作为阈值就好啦**

- 再看另一个例子

比如说，一个类似于 $Siri$ 的语音助手，我们会将语音识别的准确率作为一个最终评判的指标，但是也会出现些其他的情况，比如说，在没有说激活语句（比如 $Siri$ 是 $Hey\ Siri$ ）的时候它激活了，我们可以称之为假阳性（false positive），所以我们还会有这样一个指标，24 小时内出现假阳性的次数，这个指标的话，我们完全可以将它设定为 24 小时内最多出现一次假阳性

#### 3.1.5 Train/dev/test distributions

既然要设训练集、开发集、测试集，那么又要怎么分呢？首先，最重要的一点，一定要记住就是**同分布**

比如说，我们要做一个猫分类器，拥有着 8 个国家的数据，一个很愚蠢的做法就是拿其中四个国家的数据去做开发集、拿其余四个国家的做测试集，很明显，这是不满足同分布的，通过开发集拟合的数据当然并不能很好地拟合测试集，至于为什么不满足同分布的会导致我们有这样问题呢？看一个小例子：

我们在开发集上不断调优神经网络，使得结果不断地优化，收敛在开发集分布下的最优解下，就相当于我们以这个最优解为靶心不断地射箭，最终能够相对比较准确地射中靶心，但这时候突然把靶心移动了（测试集分布下的最优解），用它来评估准确率，这很明显是不科学的

所以，这种时候我们更多做的是，**对这些不同分布的数据进行打乱，随机调出一部分数据作为开发集，随机挑出一部分作为测试集，这样这些数据就满足同分布了**

我们在做一个项目的时候一定要将各个分布数据都考虑到，除非那个分布是实际应用中基本不会存在的

#### 3.1.6 Size of dev and test sets

嗯，我们原来可能都是这样划分 $Training Set$ 、 $Dev Set$ 、 $Test Set$ 的，比如 $Training Set 70\%\ Dev Set 30\%$ ，再比如 $Training Set 60\%\ Dev Set 20\%\ Test Set 20\%$ 这样，当然这在如今的大数据时代已经不适用了，因为原来我们只有 100~10000 这样的量级的数据，但如今我们有着上百万量级的数据，所以我们可以用更多的数据去训练，比如说 $Training Set 98\%\ Dev Set 1\%\ Test Set 1\%$，

由于测试集的作用仅仅是评估已经做好的某个神经网络，更多的数据也仅仅可以提高最终评估结果的置信度，并不能提高其准确度，所以测试集并不是需要太多的数据

有些时候会省略掉测试集，只有训练集和开发集，这样的话如果有足够自信或者真的不需要测试的情况下当然也是可以的，但最好还是设这样的一个测试集，这样可以有效地避免神经网络对开发集过拟合的现象

#### 3.1.7 When to change dev/test sets and metrics

- 举个例子，我们有两个猫分类器，第一个只有 3% 的误差，但是却有很多色情图被当做猫图推送给用户了，而第二个猫分类器则有 5% 的分类器，任何色情图都没有推送给用户，我们还有用户很明显会觉得第二个分类器比较好，但是如果按原来的指标来看，分类器一更佳，这……这说明我们**需要改变指标**了

  > 如何改变指标？我们可以将 error 的评估表达式中的色情图前面加上一个比较大的权重，这样的话，即便是很少的色情图都会造成很大的误差值，也就避免了上述情况，我们仍能使用单一误差值来评估分类器

- 另一个例子，比如说我们使用的开发集是在网上收集的质量很好的图片，而实际用户所上传的却是模糊的、摄影角度不专业的图片，这使得原来在开发集上表现优良的分类器一（error=3%）在实际应用中不如分类器二（error=5%），这时候我们需要**改变开发集**了

机器学习完全独立的两步：

1. 定义评估指标
2. 优化该指标

#### 3.1.8 Why human-level performance?

![DeepLearning15](../img/Deep_Learning/DL15.png)

大多数情况下，机器学习随着算法的优化迅速赶超人类的表现，但在超过人类表现之后，进展会越来越缓慢，但是总体上趋近于一个理论上限值——贝叶斯误差，这个上限值是无法超越的，因为总有无法识别的信息（比如说失真十分严重的信息）

为什么是在超越人类表现后进展越来越缓慢了呢？其一是因为人类表现本身已经很接近贝叶斯误差了，再者，数据的来源都是人类，所以我们有着大量人类所创造的数据来用于它准确度的提升，但想要超过人类的话，就无能为力了

#### 3.1.9 Avoidable bias

比如有一个分类器，在训练集上误差为 8% ，而在开发集上误差为 10% ，若按以前的想法我们可能会觉得，它的偏差太大了，我们应当使用降低偏差的方法（多跑几次或者使用更大的神经网络）

但如今我们知道了有贝叶斯误差，所以我们现在并不能单纯地认为 8% 是一个很大的误差了，一旦贝叶斯误差是 10% 呢？由于人类的表现已经很接近贝叶斯误差了，所以我们也可以以人类的表现来近似地替代贝叶斯误差

比如人类的表现误差为 1% ，那很明显训练集误差太大了，但如果人类的表现只有 7.5% 呢（比如说这些图片非常非常模糊，人类也很难分辨）？这时候其实训练集已经很接近贝叶斯误差了，此时我们很难继续提高训练集的准确率了，继续提高可能使其过拟合，所以我们这时候应当考虑开发集与训练集为什么会差这 2% 了，也就是考虑方差的问题了

**不过我们是怎么判断到底应该考虑偏差还是方差呢？由于原来在训练集上的误差不能单纯地使用了，我们使用一个新的参数——可避免偏差（训练集误差与贝叶斯误差之差），而方差可用开发集与训练集误差之差衡量，在第一个例子里可避免偏差为 7% ，方差为 2% ，很明显我们这时候我们应当使用降低偏差的方法，在第二个例子里可避免偏差为 0.5% ，方差为 2% ，这个时候应当使用降低方差的方法**

#### 3.1.10 Understanding human-level performance

我们如何评判人类的水平？比如说有一张放射科图片，没经过任何训练的普通人的误差可能达到 3% ，而普通医生可能达到 1% ，经验丰富的医生可能只有 0.7% ，如果这样一队经验丰富的医生相互讨论得到的结果甚至可以达到 0.5% ，但哪个是人类的水平呢？

如果是想要获得贝叶斯误差的话，很明显 0.5% 是最佳选择，因为贝叶斯误差一定比人类最佳表现更好，但如果我们只是想发表论文或者部署系统，那么一个普通医生的误差便可以了（1%）

如果我们面临训练集 5% 的误差、开发集 6% 的误差的话，我们选择哪个作为人类表现都是无所谓的，可避免偏差都比方差大；训练集 1% 的误差、开发集 5% 的误差的话，可避免偏差是一定比方差小的；但如果训练集 0.7% 的误差、开发集 0.8% 的误差，那么贝叶斯误差的近似选择就成了一个棘手的问题了，如果选择的不当，那么方向（降低方差还是降低偏差）就会错误，不过在这个时候训练集和开发集的拟合已经相当不错了，已经相当接近于人类的表现了

#### 3.1.11 Surpassing human-level performance

接着之前的例子，单人误差 1% ，一队误差 0.5% ，训练集误差 0.6% ，开发集误差 0.8% ，那么肯定是使用 0.5% 误差的啦，但如果单人误差 1% ，一队误差 0.5% ，训练集误差 0.3% ，开发集误差 0.4% ，可避免偏差是多少？贝叶斯误差选哪个值？如果贝叶斯误差是 0.5% ，那么训练集可能有 0.2% 的过拟合，但贝叶斯误差也是可能比 0.5% 低的啊，那到底是多少呢？0.4%？0.3%？0.2%？这就无从得知了……所以说这样很难评估一个与人类水平不相上下的系统

但是很多系统已经远远超过人类了，比如说广告的推送、物流时间等等，不过这些都只是通过**结构化数据训练**出的，因为他们有着大量数据的支持，**很容易超过人类**，而在类似语音识别、图片识别等**自然感知任务**方面，**超过人类还是很难的**

#### 3.1.12 Improving your model performance

下面就是总结咯

我们的步骤是，第一步要设计合适的网络使得它在训练集上的偏差足够小，也就是尽量减小可避免偏差；第二步就是利用开发集和测试集优化，使得对开发集与测试集的误差尽量得小，也就是尽量减小方差

1. 减小可避免偏差，我们经常用的策略有

   - 训练更大规模的 NN / 训练更久
   - 选择更好的优化算法（RMSprop、momentum、Adam）
   - 选择其它的神经网络框架（CNN、RNN） / 优化超参数（层数、隐藏单元数等等等等）

2. 减小方差，我们经常用的策略有

   - 收集更多的数据
   - 正则化（$L2$ 正则化、$dropout$ 正则化、数据增强）
   - 选择其它的神经网络框架 / 优化超参数 （同上）

### 3.2 ML Strategy (2)

#### 3.2.1 Carrying out error analysis

如何判断一个算法到底出错在哪里了呢？emmmmm，那我们可以看看它在哪里犯下错误了

比如说，我们这么一个猫分类器，然后它取得了 90% 的准确率，然后我们就要分析那 10% 的错误到底是怎么错的

比如这 10% 有相当一部分是由于把一些狗给分类成猫了，那么我们可以收集些狗的图片用于训练，但如果说狗只占一小部分的话，那么这样就显得很没必要了，所以我们应当考虑到分类错误的各种情况，用一个表格列出来，统计下影响比较大的那几项，并针对其进行处理就可以啦，比如这里我们可以考虑下是否是猫科动物、是否是狗、是否是太模糊、甚至可能是某种滤镜出了问题

#### 3.2.2 Cleaning up Incorrectly labeled data

数据中无法避免有一些是被标记错误的，比如说一只猫在一个非常角落的地方不小心漏掉了，再或者把一只画中的猫标记成了真猫，那我们如何处理这些标记错误的数据呢？

如果说标记错误地比较有随机性的话，那么算法的鲁棒性会弥补这个错误，但如果说出现了把所有白色的狗都标记成猫这样的严重错误的话，很明显，最终的算法也会把白色的狗给标记成猫，这个时候我们不得不花些时间去修正它们了

我们可以像上一节一样，将出错部分的数据的出错原因一一列出，也把标记错误这个原因列出来，一样地，我们看下这个原因所占比例如何，如果比例不高，我们可以优先优化其他问题

当然，刚刚对错误的评估仅仅是针对开发集，但如果我们真的要去修正的话，**最好开发集和训练集都修正，以保证他们满足同分布**，不过仅仅修正开发集也是可以的，因为其分布的差别应当不会特别大

另外，我们**修正的时候不要仅仅修正这些发生错误的部分，也要对正确那部分进行修正**，因为那些正确的部分也许是侥幸分类正确的啊……

#### 3.2.3 Build your first system quickly, then iterate

如果要构建一个全新的机器学习应用的话，最好的方法并不是考虑的太多（这会使得系统一建立便过于复杂），最好是**快速划分训练集、开发集，然后迅速建立一个简单的神经网络，并用上周所说的偏差/方差评估法对其进行评估**，在评估过程中我们会找出算法的最大缺陷的，然后我们针对该缺陷进行改进

#### 3.2.4 Training and testing on different distributions

训练神经网络是要很多很多的数据的，但我们很难收集到和实际应用分布相同的数据，所以我们会尽可能地收集相似的数据，尽管它们的分布与实际应用时差别很大

比如我们要做一个猫分类器，我们从网络上爬取到取景专业、分辨率高的图片（收集到了 20w 组），而我们实际应用的数据（从 APP 上收集的数据）很模糊、取景不专业（也收集到了 1w 组），那要如何划分数据集呢？下面有两种方案，让我们看下

- 将两种数据随机打乱后，$Train\ Set\ 21w$ ，$Dev\ Set\ 5k$ ， $Test\ Set\ 5k$ ，嗯，都来自同分布了，但是缺点很致命，因为它们与我们的最终目标相差甚远，因为 APP 数据只占很小一部分，所以说我们一直瞄着的方向和我们最终应用的方向会差很多很多，还是看下一种方法吧

- 训练集中包含全部 20w 爬取数据，此外包含 1w APP 上的数据，而开发集、测试集各 5k 的 APP 上的数据，这就保证了我们后来的目标与最终目标是一致的，只不过训练集和开发集间的分布会相差较大些，但这种方法确实会带来更好的系统性能，至于训练集与开发集分布的差异，后续会讲到优化方案

再举个例子，比如我们要做一个车载语音系统，但真正车载语音数据仅有 2w ，而其他语音系统的数据可以买到 50w 这么多，所以我们可以训练集中包含这 50w 其他数据，而开发集测试集各 1w 车载语音数据，或者使用 50w 其他数据与 1w 车载数据作为训练集， 开发集测试集各 5k 这样

#### 3.2.5 Bias and Variance with mismatched data distributions

我们之前都是针对分布相同的数据集评估偏差与方差，那么分布不同的呢？

我们知道，我们原来使用训练集误差与贝叶斯误差作为可避免偏差，开发集误差与训练集误差作为方差处理，这样可以有效判断到底问题出在方差还是误差上，但分布不同的话，开发集误差与训练集误差的差值可就不仅仅是方差的问题了，可能其还包含了两种数据之间不匹配的问题（就是向两个靶心之间的差距啦，靶心不同肯定会有误差的），那如何评估数据不匹配问题和方差问题呢

我们知道，我们原来开发集与测试集同分布，因而我们不需要考虑数据不匹配的问题，相同地，我们可以找一个和训练集分布相同的来评估下方差问题，故从训练集中抽出一部分数据作为训练-开发集，这部分数据不参与反向传播，这保证了训练-开发集误差与训练集误差之差就是方差，而训练-开发集误差与训练集误差之差就是数据不匹配问题

所以，我们现在需要考虑的就是：

- 贝叶斯误差（人类误差） ---- 可避免偏差 ---- 训练集误差
- 训练集误差 ---- 方差 ---- 训练-开发集误差
- 训练-开发误差 ---- 数据不匹配问题 ---- 开发集误差
- 开发集误差 ---- 开发集过拟合问题 ---- 测试集误差

有时候会出现一些有趣的事情，比如说开发集与测试集误差比训练集误差小，其实这是可能的事情，比如说上节的语音识别问题，开发集与测试集比训练集更容易识别是完全可能的，我们看下这种情况下要如何进行分析：

我们制作一张二维表格，水平轴上写下各种数据集，比如说是买来的普通语音数据还是收集的车载数据，竖直轴上写下处理数据的不同方法和算法，首先是人类的水平误差，其次是神经网络训练过的数据集上的误差，最后是神经网络未训练过的数据集上的误差

![DeepLearning16](../img/Deep_Learning/DL16.png)

(1, 1) 位置自然就是人类水平， (2, 1) 位置自然就是训练集误差， (3, 1) 位置自然就是训练-开发集误差， (3, 2) 位置自然就是开发集误差，他们之间的差值的意义和前面所述一致

而 (1, 2) 位置和 (2, 2) 位置也是有用的， (1, 2) 位置就是人类在车载语音方面的识别误差， (2, 2) 位置就是利用车载语音训练出来的神经网络的误差，我们这样将整个表格填出来之后可以观察到更多的特征，这样就更加方便对神经网络的评估啦

#### 3.2.6 Addressing data mismatch

前面提到了数据不匹配这个问题，不过这个问题并没有什么比较有效地解决方案，但我们有一些尝试可以去做一下

首先我们做下错误分析，还是以车载语音助手为例，训练集与开发集之间的差异有可能是汽车的噪音造成的，另外可能发现有很多对街道号识别错误的问题，我们可以从这些问题着手进行优化

比如说噪音问题比较大，那么我们可以在将噪音合成在原数据上，再比如说街道号识别问题比较大，我们可以多增加些人工读数字的数据到训练集

值得注意的是，如果我们使用 1 个小时的车载背景噪音合成在 1w 小时的普通语音数据上会存在一个潜在问题就是，神经网络很可能对这 1 小时的背景音过拟合，虽然我们并不能听出来这一小时背景音是循环使用的，但神经网络可是很容易做到的，所以可以尽量收集更多的背景音以预防这种情况

#### 3.2.7 Transfer learning

如果我们有一个已经训练好的图片识别系统，我们已经知道，它比较低的层次实现了对点、线、边缘等的识别功能，所以我们完全可以将这个神经网络迁移到一个放射科图片检测系统中，比如将输出层及其参数删掉，更换新的输出层或者增加几层再输出，然后利用放射科图片对这部分神经网络进行优化，这样可以更快地训练好放射科神经网络，因为神经科图片数据并非像普通图片数据那么容易获得

迁移学习一定要注意，首先**输入一定是相同的**，这个例子里的输入都是图片，这样保证了前几层的处理效果都是一样的（点线边缘等等），再就是**原任务比迁移后的任务数据量大很多**，否则的话就失去了迁移学习的意义了

这种学习方法的两次训练之间是串行的，下面介绍并行的学习方法

#### 3.2.8 Multi-task learning

比如我们要研究无人驾驶技术，那么我们需要同时检测多种物体，比如说，行人、车辆、停车标记、红绿灯等等，那么我们最终的输出层就类似于 `[0, 1, 0, 1]`，当然这不是 Softmax 分类器，因为这里可能同时出现多个 `1` ，所以我们的最终的损失是 $\frac{1}{m} \sum_{m=1}^m \sum_{j=1}^4 L(\hat{y}_j^{(i)}, y_j^{(i)})$ ，我们的数据集中可能出现`[1, 1, 0, ?]` 的情况，也就是没有对是否有红绿灯打标签，那么我们在求和时就忽略那一项

这种方法比之于分成 4 个神经网络分别识别四种物体的好处是，他们共用相同的低层次网络与参数，这样他们会相互学习，使得性能更高，实践也表明了，在神经网络足够大的情况下，这种方法比分开识别的性能更高

当然它也是有适用条件的，首先**同时训练的这几组任务可以共用低层次特征**，其次，**对于某个要专注提升性能的任务，其余所有任务最好超过该任务样本数**，最后，前面提到了，**多任务是在神经网络比较大的情况下效果更好**

通常，人们更多使用学习迁移而较少使用多任务学习，也许因为很难找到那么多相似且数据量对等的任务可以用单一神经网络训练，但他们确实都是比较实用的工具

#### 3.2.9 What is end-to-end deep learning?

端对端是指用单个神经网络代替从前多个阶段处理的系统

比如说，我们原来的一个语音识别需要输入语音、提取特征、寻找音位、组成单词最终译成文本，这使得我们很容易获得任务，但如果使用端对端方法，我们可以输入语音、通过一个神经网络、直接输出文本

再举个例子，比如说门禁的人脸识别系统，如果使用端对端的话，应当是输入门禁捕捉到的图片，然后喂入网络识别，但是事实上我们可以分为简单的两步取得更好的效果，首先用软件标记出人脸的位置，然后将人脸裁剪出来并喂入神经网络进行识别，之所以分成两步更加好，因为我们有很多的人脸标记位置的数据与人脸匹配的数据，所以我们可以很方便地建立这两步，但直接一步成型的话，至少现在的效果还是不好的

所以说，端对端并不一定是最好的方法，具体哪个更好我们还需要自己仔细分析下

#### 3.2.10 Whether to use end-to-end learning?

是否要使用端对端学习呢？我们先看下它的优缺点：

优点：

- 更多由数据来说话，没有过多的人为因素干预
- 人工设计部分更少了

缺点：

- 不可避免地，我们需要很多很多的数据来训练它
- 无法采用一些人工设计，因为有些人为干预会有很大的帮助

至于到底要不要用端对端学习，首先我们可能是要看是否有足够的数据来构建端对端学习网络，因为这真的需要很多很多的数据来完成

## 4 Convolutional Neural Networks

### 4.1 Foundations of Convolutional Neural Networks

#### 4.1.1 Computer vision

深度学习给计算机视觉带来了极大的突破，计算机视觉中有着不少其他领域可以借鉴的方法

我们之前都是讨论的 (64, 64, 3) 这样的小图，如果我们有 (1000, 1000, 3) 这样的大图呢？直接使用全连接网络的话需要的参数可是……好多好多，所以下面我们就要考虑下卷积操作了

#### 4.1.2 Edge detection example

我们之前了解到，神经网络的前几层是对低层次结构进行检测，比如人脸识别神经网络的前几层会检测边缘、区域等等，下面我们看下边缘检测是如何进行的

![DeepLearning17](../img/Deep_Learning/DL17.png)

过滤器（卷积核）在原图上滑动，计算出新图，`*`表示卷积

其实这个卷积处理计算了竖直边缘，为啥呢？看下面这个例子

![DeepLearning18](../img/Deep_Learning/DL18.png)

#### 4.1.3 More edge detection

- 竖直边缘检测（左亮右暗为正值）

$$
\begin{bmatrix}
1 & 0 & -1\\
1 & 0 & -1\\
1 & 0 & -1
\end{bmatrix}
$$

- 水平边缘检测（上亮下暗为正值）

$$
\begin{bmatrix}
1 & 1 & 1\\
0 & 0 & 0\\
-1 & -1 & -1
\end{bmatrix}
$$

- Sobel 过滤器

$$
\begin{bmatrix}
1 & 0 & -1\\
2 & 0 & -2\\
1 & 0 & -1
\end{bmatrix}
$$

- Scharr 过滤器

$$
\begin{bmatrix}
3 & 0 & -3\\
10 & 0 & -10\\
3 & 0 & -3
\end{bmatrix}
$$

- 你自己的过滤器（作为一个参数，在反向迭代中自动优化）

$$
\begin{bmatrix}
w_{1} & w_{2} & w_{3}\\
w_{4} & w_{5} & w_{6}\\
w_{7} & w_{8} & w_{9}
\end{bmatrix}
$$

#### 4.1.4 Padding

很明显，每次卷积之后图片都会变小，那么经过多次卷积之后的图片就会变得非常小了

比如说，一个 `(6, 6)` 的图片经 `(3, 3)` 的过滤器后，会产生 `(4, 4)` 的图片，一般地，就是说 `(n, n)` 的图片经 `(f, f)` 的过滤器后会产生 `(n-f+1, n-f+1)` 的图片，为了不使图片变小，我们可以对图片进行填充处理（Padding），比如说在图片最外层填充 `p` 圈像素点（一般填充 0 ）

常用的 Padding 方法有

- Valid 卷积，不填充 `p = 0`
- Same 卷积，卷积前后的图片大小不变，很容易得知 $p = \frac{f - 1}{2}$ ，通常情况下 $f$ 是奇数

#### 4.1.5 Strided convolutions

我们之前过滤器每次只移动一个像素，如果移动多个像素，那么就称其为步长 $strid$

很容易地，我们知道这个时候经过卷积后的图片应当是变成了 $(\frac{n-f+2p}{s} + 1, \frac{n-f+2p}{s} + 1)$ ，当商不是整数的时候，我们使用向下取整 $(\left\lfloor \frac{n-f+2p}{s} \right\rfloor + 1, \left\lfloor \frac{n-f+2p}{s} \right\rfloor + 1)$ ，也就是说，当过滤器一部分移到原图外的时候不作计算

另外，在数学中卷积的定义是先对过滤器水平镜像翻转与竖直镜像翻转，再放到原图上滑动计算，在深度学习实践与文献中一般省略掉了镜像的过程，省略掉镜像的过程在数学上一般被叫做互相关，但深度学习中一般便称其为卷积操作了

#### 4.1.6 Convolutions over volumes

![DeepLearning19](../img/Deep_Learning/DL19.png)

前面的图片都是灰度图片，只有一个颜色通道，但我们的图片往往是 $RGB$ 三个颜色通道的，这个时候我们的输入增加了一个维度（通道数， $channels$，$n_c$），相应地，过滤器也是要变成三维的，而且他们的通道数相同，这里都是 3 ，至于运算方法，和原来一样，滑动乘积就好，当然输出会是一个二维的

如果我们只关心红色通道的竖直边缘的话，我们可以将过滤器设为

```python
R :
[
   [1, 0, -1],
   [1, 0, -1],
   [1, 0, -1]
]
G :
[
   [0, 0, 0],
   [0, 0, 0],
   [0, 0, 0]
]
B :
[
   [0, 0, 0],
   [0, 0, 0],
   [0, 0, 0]
]
```

若只关心竖直边缘而不关心具体是那个通道下的边缘的话，RGB 都设为

```python
[
   [0, 0, 0],
   [0, 0, 0],
   [0, 0, 0]
]
```

就好

那么，我们想要同时获取多个特征怎么办，比如说既想要垂直边缘也想要水平边缘

![DeepLearning20](../img/Deep_Learning/DL20.png)

我们可以使用多个过滤器，各个输出叠在一起，形成一个三维的输出方块，这里使用 2 个过滤器，输出的第三维自然也是 2

#### 4.1.7 One layer of a convolutional network

![DeepLearning21](../img/Deep_Learning/DL21.png)

和之前一样，前向传播是要 $z^{[1]} = W^{[1]} a^{[0]} + b^{[1]}$ ，之后增加非线性函数使得 $a^{[1]} = g(z^{[1]})$ 这里 $a^{[0]}$ 就是输入的 $(6, 6, 3)$ 图片，经由两个 $(3, 3, 3)$ 的过滤器，或者说 $(3, 3, 3, 2)$ 的过滤器，生成 $(4, 4, 2)$ 的图片，这就得到了 $W^{[1]} a^{[0]}$ ，之后利用 Python 的广播特性对每个元素进行处理就得到了 $a^{[1]}$

如果是一个全连接网络的话，我们当时讨论过那时的参数取决于输入结点数，图片像素点越多，参数自然越多，但使用卷积完全不用担心这个问题，假设我们使用 $(3, 3, 3, 10)$ 的过滤器，那么只有 $(3 \times 3 \times 3 \times 3 + 1) \times 10 = 280$ 个参数，无论这张图片有多少像素点，参数数量只有这么多的

我们来看下我们现在在 $l$ 层各个参数分别是多少

首先 $f^{[l]}$ 是过滤器的长（宽），$p^{[l]}$ 是 padding 处理程度， $s^{[l]}$ 是步长， $n_c^{[l]}$ 是过滤器的个数

我们的输入图片为 $n_H^{[l-1]} \times n_W^{[l-1]} \times n_c^{[l-1]}$
输出图片为 $n_H^{[l]} \times n_W^{[l]} \times n_c^{[l]}$

由于输出图片是输入图片经过卷积得到的，所以他们之间的关系有

- $n_H^{[l]} = \left\lfloor \frac{n_H^{[l-1]} + 2p^{[l]} -f^{[l]}}{s^{[l]}} + 1 \right\rfloor$
- $n_W^{[l]} = \left\lfloor \frac{n_W^{[l-1]} + 2p^{[l]} -f^{[l]}}{s^{[l]}} + 1 \right\rfloor$

由于过滤器的通道数取决于输入图片通道数，所以过滤器 $(f^{[l]}, f^{[l]}, n_c^{[l-1]}, n_c^{[l]})$

$b^{[l]} \rightarrow (1, 1, 1, n_c^{[l]})$

另外，由于 $a^{[l]} \rightarrow (n_H^{[l]}, n_W^{[l]}, n_c^{[l]})$ ，故 $A^{[l]} \rightarrow (m, n_H^{[l]}, n_W^{[l]}, n_c^{[l]})$

#### 4.1.8 A simple convolution network example

![DeepLearning22](../img/Deep_Learning/DL22.png)

卷积、卷积、卷积、展开

首先，前面的卷积层使得长宽不断缩小，不断增加的过滤器使得通道数不断增加

之后展开，利用 logistic or softmax 输出

不过一般情况下卷积神经网络都由以下三层组成的：

- 卷积层 $CONV$
- 池化层 $POOL$
- 全连接层 $FC$

#### 4.1.9 Pooling layers

池化层可以缩减模型的大小，提高计算速度并且可以提高所提取特征的鲁棒性

- 最大池化

  ![DeepLearning23](../img/Deep_Learning/DL23.png)

  有点类似卷积时过滤器的移动，两个超参数 $f$ 和 $s$ ，这里 $f = 2, s = 2$ ，也就是$(2, 2)$ 的过滤器以 2 为步长进行移动，每次计算出 $(2, 2)$ 方块内的最大值

  直观理解大概是，如果这一通道提取的是竖直边缘的话（上图中左上角有，右上角没有），经过提取之后的结果不会过多地损失原来的特征（还是左上角有，右上角没有）

- 平均池化

  很少用到，不用说也猜到了，就是每次计算平均值而不是最大值，但在比较深层的神经网络终会遇到

当输入为多个通道时，每个通道也是单独计算的，这和卷积那里一样的

很明显，这里有两个超参数 $f$ 和 $s$ （一般取$f = 2, s = 2$ 或 $f = 3, s = 2$），$padding$ 基本用不到，另外可以考虑的还有使用最大池化还是平均池化

#### 4.1.10 Convolutional neural network example

![DeepLearning24](../img/Deep_Learning/DL24.png)

![DeepLearning25](../img/Deep_Learning/DL25.png)

图片的长度与宽度越来越小，但信道数越来越高，而且参数大多在全连接层，卷积层只有很少的参数

这里的超参数设置的最好方法是阅读更多的案例

#### 4.1.11 Why convolutions?

- 参数共享 很明显，在过滤器滑动过程中有很多输入值共享同一过滤器的参数

- 稀疏连接 在过滤器停在某一位置时，其输出值与输入值之间的连接就是他们 $(f, f, n_c)$

另外，卷积操作有着很好的平移不变属性，通过反向迭代可以获得更好的平移不变属性

### 4.2 Deep convolutional models: case studies

#### 4.2.1 Why look at case studies?

在一个任务上表现良好的卷积神经网络往往在其他任务上表现也良好，比如有一个很好的识别猫、狗、人的神经网络，利用它来训练自动驾驶，效果往往也是很好的，至少也是有很大的借鉴价值的

下面的几节课就围绕几个经典的神经网络进行展开

- Classic networks
  - LeNet-5
  - AlexNet
  - VGG
- ResNet
- Inception

#### 4.2.2 Classic networks

- LeNet-5

  ![DeepLearning26](../img/Deep_Learning/DL26.png)

  1998 年，有很多和现在所使用的不太一样，比如：

  - 当时并不使用 Padding ，所以每次卷积都会变小
  - 那时候更多使用 avg pool 而不是 max pool
  - 激活函数主要使用 sigmoid 和 tanh 而不是 ReLU
  - 那时候最后的分类函数并不是 Softmax 而是别的什么，现在已经不常用了

* AlexNet

  ![DeepLearning27](../img/Deep_Learning/DL27.png)

  - 使用了 Padding
  - 使用了 ReLU
  - 最后使用了 Softmax

* VGG-16

  ![DeepLearning28](../img/Deep_Learning/DL28.png)

  - 每次都使用 $2 \times 2$ 池化，长宽各减半
  - 过滤器通道数 64、128、256、512、512，通道数翻倍

#### 4.2.3 Residual Networks (ResNets)

残差网络对深层神经网络有着很好的优化作用，它可以很好地改善梯度爆炸、梯度消失等问题

- 首先看残差块

  ![DeepLearning29](../img/Deep_Learning/DL29.png)

  很容易看到，我们原来 $a^{[l+2]} = g(z^{[l+1]})$ 变成了 $a^{[l+2]} = g(z^{[l+1]} + a^{[l]})$ ，也就是 $a^{[l]}$ 通过“捷径”（又称跳远连接）迅速到达更深层的位置

- 然后我们再看残差网络，很明显，就是一系列残差块所构成

  ![DeepLearning30](../img/Deep_Learning/DL30.png)

  理论上，我们深度越深的神经网络误差会越小，但是深度越深的神经网络越难以训练，会出现一些错误，这使得出现上图左侧蓝线中的情况，而残差网络恰恰优化了这一问题，使得深层神经网络也可以保证良好的性能

#### 4.2.4 Why ResNets work?

首先，从一个大型神经网络开始

![DeepLearning31](../img/Deep_Learning/DL31.png)

在此基础上我们加两层，并使用一个跳远连接，将其作为一个残差块处理，很明显 $a[l+2] = g(z^{[l+2]} + a^{[l]}) = g(W^{[l+2]} a^{[l+1]} + b^{[l+2]} + a^{[l]})$ ，我们假设 $W^{[l+2]} = 0, b^{[l+2]} = 0$ ，也就是说 $a^{[l+2]} = g(a^{[l]})$， 如果我们使用 ReLU 激活函数，那么 $a^{[l+2]} = a^{[l]}$ ，也就是说加上的这两层没有起作用

没有起作用说明神经网络很容易学习到未加这两层时候的效果，这就使得加上这两层后并不会使得原来的输出彻底打乱，而是比较好地保留了原来的结果

另外，值得注意的是，$z^{[l+2]}$ 和 $a^{[l]}$ 需要同维度才可以相加，所以这里面有着大量的 Padding 以保证维度不变，如果维度真的不一样，可以使用 $W_s a^{[l]}$ 替代 $a^{[l]}$ 以保证维度不变

#### 4.2.5 Network in Network and 1×1 convolutions

$1 \times 1$ 卷积有什么用呢？如果只有一个通道的话，那么输出也就是整体放大一定的倍数而已，但如果是多个通道呢？

![DeepLearning32](../img/Deep_Learning/DL32.png)

我们可以发现，各个输入信道与各个输出信道的同一位置建立起了全连接，之后还有一个 $ReLU$ 激活

但是它有什么用呢？

我们平时想压缩长宽可以通过 Pooling 很容易做到，但压缩信道数要怎么做呢？因为 $1 \times 1$ 卷积可以理解为每个位置上各个信道间建立全连接，所以很容易将信道数进行压缩，当然如果我们想增加或者保持信道数也是可以的

#### 4.2.6 Inception network motivation

构建卷积网络难免要遇到对过滤器大小的选择问题（$1 \times 1$ 还是 $3 \times 3$ 还是 $5 \times 5$），还有要不要加池化层的问题， Inception 网络可以很好地为我们解决该问题

![DeepLearning33](../img/Deep_Learning/DL33.png)

首先，我们使用一个 $1 \times 1$ 的过滤器得到一个输出，然后我们再使用一个 $3 \times 3$ 的过滤器得到一个输出，叠在 $1 \times 1$ 输出上，当然，这需要对 $3 \times 3$ 进行 same Padding ，$5 \times 5$ 也使用相同的处理方法，最后再叠上一个 MAX-POOL 层，这就是一个 Inception 层

Inception 有一个问题就是计算成本的问题，我们可以通过计算其中的乘法次数来计算

![DeepLearning34](../img/Deep_Learning/DL34.png)

这里考虑 $5 \times 5$ 卷积这步，共 $32$ 个过滤器，每个过滤器在每个位置计算 $5 \times 5$ 次乘法，每个通道 $28 \times 28$ 个位置，共有 $192$ 个通道，乘在一起约为 1.2 亿

![DeepLearning35](../img/Deep_Learning/DL35.png)

这里还有另一种方法，就是先对其进行 $1 \times 1$ 卷积减小其通道数，再进行 $5 \times 5$ 卷积，按照刚才的方法，将两次的乘法数加在一起，结果仅仅是 1204 万，大大减小了计算的成本

刚刚使用的方法中间那层可以称为瓶颈层，通过该层可以很好地降低计算成本，而且事实表明其也不会降低网络性能

#### 4.2.7 Inception network

![DeepLearning36](../img/Deep_Learning/DL36.png)

按照上面的思路，$3 \times 3$ 和 $5 \times 5$ 都增加一个 $1 \times 1$ 层以减少计算成本，最大池化后也加一个 $1 \times 1$ 以降低通道数，这就形成了一个 Inception module

那整个 Inception Network 是如何建立的呢？

![DeepLearning37](../img/Deep_Learning/DL37.png)

很明显，这是由一个一个的 Inception Network 组成的，只不过某些地方有些最大池化层，而且在中间的隐藏层有两处直接引出、经过全连接层、Softmax 得出结果，它确保了即便是隐藏单元和中间层也参与了特征计算，起到一种调整的效果，并且能防止网络发生过拟合

#### 4.2.8 Using open-source implementations

很多优秀的神经网络有着开源实现，比如[残差网络论文原作者对其的实现](https://github.com/KaimingHe/deep-residual-networks)

#### 4.2.9 Transfer Learning

**相比于随机初始化权重，我们直接下载别人训练好的权重用来初始化会有着更好的效果**

计算机视觉的研究社区非常喜欢把许多数据集上传到网上，如果你听说过，比如 **ImageNet**，或者 **MS COCO**，或者 **Pascal** 类型的数据集，并且有大量的计算机视觉研究者已经用这些数据集训练过他们的算法了

比如我们建立一个猫咪检测器，用来区分 猫 Tigger、猫 Misty，很明显这是一个三分类问题，不考虑 Tigger 和 Misty 同镜的情况的话，有着 Tigger 、 Misty 、 other 三种情况，可以最后使用 Softmax 进行输出，我们可以从 ImageNet 上进行迁移

![DeepLearning38](../img/Deep_Learning/DL38.png)

当我们数据量比较少的时候，我们可以将其网络和权重都拿来，拿掉最后一层换成我们自己的 Softmax 层，利用该权重初始化其它层并且冻结起来，也就是并不更新其参数（只需要根据框架传入参数就可以啦，比如 `trainableParameter=0`、`freeze=1`这样），只更新最后一层的参数

> 这里有一个技巧，因为前面冻结住的几层的变换是固定的，所以我们可以直接将输入数据经过冻结层预处理下，存在硬盘上，这样就不用每次训练都运行该变换了，可以降低训练时间

当我们有稍多一点的数据的时候，我们可以尝试将几个冻结层取消冻结，让其参数变成可更新，甚至我们重新搭建这几层，也就是让可学习的部分变得更多些

数据再多些，我们可以干脆只用其参数做初始化，全部参与训练

#### 4.2.10 Data augmentation

计算机视觉需要用到大量的数据，所以我们很自然地想到对数据的扩增，方法前面也有简单提到过

这里依然以猫分类器为例

- 水平镜像 猫翻转后还是猫
- 随机裁剪 从原图中随机剪裁出一部分图片
- 色彩转换 对颜色略微做下变换，比如 $RGB$ 分别 $(-20, +20, +20)$ ， $G$ 和 $B$ 的增加使得颜色略偏黄，可以模拟阳光的效果，当然这里 $RGB$ 的变换程度满足一定分布而不是固定的值，另外可以试下 $PCA$ 变换以更好地实现色彩转换

原图是保存在硬盘上的，我们通常用一个或多个 CPU 线程对图片进行增强处理，形成一个 $batch$ 或者 $mini-batch$ ，然后将其交给 CPU 或 GPU 的线程来训练，这使得增强和训练可以并行

#### 4.2.11 The state of computer vision

很明显，当我们有足够多的数据的时候，我们所需要做的手工操作会比较少，我们会花更多的时间去构建比较好的的网络

但我们数据没有那么多的时候，我们需要考虑如何精心的设计它的特征，但这样会增加人工干预（可参考 #3.2.10）

::: tip

一些能在基准测试中表现更好的技巧：

- 集成 独立训练多个网络，并平均化他们的输出
- $Multi-crop$ 将数据扩充，比如 $10-crop$ ，是取原图中间、左上、右上、左下、右下五张图以及他们的镜像图组成的 10 张图来测试

但这些技巧并不适合用在生产系统上，尽管其在基准测试和竞赛上做的很好

:::

很明显，我们的计算机视觉是建立在小数据集之上的，但是其他人已经完成了很多的手工工程了，一个神经网络往往在其他问题上也是很有效的

所以，如果我们想要构建自己的实用系统，最好是先从别人的框架入手，当然，_如果我们想要发明自己的计算机视觉想法的还是要自己从头开始做的_

### 4.3 Object detection

#### 4.3.1 Object localization

如何做到目标定位？

很明显，现在我们不仅要输出图片内有什么东西，还要输出这个东西在哪，比如说一个自动驾驶系统，我们需要检测的对象有：

- 行人
- 其他车辆
- 摩托车
- 仅有背景

很明显，如果说是分类问题的话，这就是简单的四分类问题，但如何额外输出对象的位置呢？简单的想法就是……额外加输出，这里我们需要的那个框需要位置和大小，也就是中心坐标 $(b_x, b_y)$ ，高 $b_h$ ，宽 $b_w$ （图片左上角 $(0, 0)$ ，右下角 $(1, 1)$）

综上，我们现在需要 8 个输出的参数，四分类四个输出、中心坐标、高宽，或者可以这样看，我们把四分类中的是否仅背景作为参数 $p_c$ ，如果未检测到对象为 0 ,如果有对象则为 1 ，其余不变，也就是

$$
y = \\
\begin{bmatrix}
p_c \\
b_x \\
b_y \\
b_h \\
b_w \\
c_1 \\
c_2 \\
c_3 \\
\end{bmatrix}
$$

我们的标签的内容也是这样，只不过我们不会关心 $p_c = 0$ 的时候其他值到底是多少，这体现在计算损失函数的时候，比如使用平方误差策略，当图片内有图片对象的时候，损失值就是各元素误差的平方和，而当图片内无对象的时候，损失值只需要计算 $(\hat{y_1} - y_1)^2$ 即可，因为我们不需要关心在图片中没有元素的时候的其他输出值大小

当然，实际应用中更多是对边界框坐标使用平方误差或类似方法、对 $p_c$ 应用逻辑回归函数或者平方误差函数

#### 4.3.2 Landmark detection

类似于上节内容，我们现在不仅要识别是否有人脸，而且要识别这个人脸的具体特征，首先我们试着将人的右眼角坐标输出，类似于前面内容，我们只需要额外输出一个坐标 $(l_x, l_y)$ 就好了

那我们再加些点，比如四个眼角都输出，这样有助于我们识别眼睛的位置，也就是 $(l_{1x}, l_{1y})$, $(l_{2x}, l_{2y})$, $(l_{3x}, l_{3y})$, $(l_{4x}, l_{4y})$

我们也可以输出嘴角或者嘴上的各个点，这样就可以获得是微笑还是什么表情了

同样地，我们可以从脸上找到 64 个或者其它数量的点，这样就完全可以获得整张面部的表情数据了，也就是我们需要输出 129 个数据，当然，其中一个是那个是否有脸的数据（当然，当没脸时，其他数据也就没有意义了）

另外值得注意的一点是，标签中数据的顺序不能错，如果第一个数据是右眼角的 $x$ 坐标，那么所有数据标签的第一个数据都应该是右眼角的 $x$ 坐标

#### 4.3.3 Object detection

这节课讲解的是基于滑动窗口的目标检测算法，具体实现呢……听名字就能猜到啦

首先，我们的训练集的输入是已经裁剪好的图片，刚好将汽车给裁剪出来，而输出就是这个小图块是否有车（0 or 1）

然后我们先拿一个小图框以一定步幅在待检测图上不断遍历，没有车就会输出 0，有车就输出 1

之后再拿大一些的图框以及更大的图框遍历，待遍历完整张图之后，是否有车、车在哪里自然也就知道了

这种方法的缺点一目了然，就是实在太慢，要把整张图遍历一遍

#### 4.3.4 Convolutional implementation of sliding windows

我们很容易想到，滑动窗口的实现过程中有很多重复计算的地方，因为裁剪的时候是有很多交集的

所以我们需要改进下算法啦，不过在那之前，我们先学下如何使用卷积网络实现全连接层

![DeepLearning39](../img/Deep_Learning/DL39.png)

首先，图的上半部分很明显是我们原来使用的卷积+全连接神经网络，现在我们在全连接这里稍稍变换下，比如卷积层最后输出 $5 \times 16 \times 16$ 的图片，如果直接全连接就是线性化输出 400 个结点，现在我们使用 $5 \times 5 \times$ 的 filter 对其卷积，也就是和图片一样大的 filter ，很明显，这样会将所有像素点变换成一个像素点，当然，我们可是要使用 400 个 filter 的，这样就可以输出 $1 \times 1 \times 400$ 的结点啦

下一步，使用 $1 \times 1$ 卷积，我们已经知道这就相当于在信道之间建立全连接层，而这个时候像素点可是 $1 \times 1$ 的啊，这就和真正的全连接层完全一样啦，如此重复几次完成原来 FC 的任务，最后的输出也是一样哒

![DeepLearning40](../img/Deep_Learning/DL40.png)

我们现在考虑从 $16 \times 16$ 的图片中以 $2$ 为步长计算 $14 \times 14$ 小图块的情况，上图上半部分是我们对每个小图块的处理情况，我们用相同的方法处理整张图片的话，会得到一系列输出，我们很容易看到，最后左上角的小输出就是对应了原图左上角图块的输出，另外三个自然就是其他三个小图块的输出啦

![DeepLearning42](../img/Deep_Learning/DL42.png)

![DeepLearning41](../img/Deep_Learning/DL41.png)

最后我们再看一下这个实现过程，$28 \times 28$ 的图片经过上面的卷积后生成了 $8 \times 8 \times 4$ 的输出，也就分别对应下面图片上各个小图块的计算结果，只不过这样我们避免了对重复部分的计算，提高了计算效率，不过它还有一个问题就是边界框的位置可能不太准确（只考虑哪个框里有而没考虑到底在哪个具体的位置）

#### 4.3.5 Bounding box predictions

我们现在试下新的算法——YOLO(You only look once) 算法

![DeepLearning43](../img/Deep_Learning/DL43.png)

首先，将图分成几个部分，比如这里是 $3 \times 3$ 份（实际应用中一般更精细，比如 $19 \times 19$）

之后，我们在每个小方块上运行定位算法，也就是 4.3.1 中所述的方法，很明显，我们的每个格子的输出都应该是 8 维的向量，整张图的输出就应该是 $3 \times 3 \times 8$

但对于标签数据我们需要再说明几个地方，$p_c$ 自然是该小方块内有没有目标，不过这里我们只考虑目标的中心所在的那个方块，只将该方块标记为 1 ，$b_x, b_y$ 很明显就是中心的位置啦，只不过现在以该方块左上角为 $(0, 0)$ 右下角为 $(1, 1)$ ，它们自然是在 0 和 1 之间（所以这里我们可以使用 sigmoid），而 $b_w$ $b_h$ 就不一样啦，他们是可以大于 1 的（都是正数，可以使用指数参数化）

另外，值得注意的一点就是， YOLO 算法是只运行一次的，也就是一次卷积对 9 个图块同时计算，所以速度非常快

#### 4.3.6 Intersection over union

![DeepLearning44](../img/Deep_Learning/DL44.png)

我们需要一个度量识别准确率的参数，这样可以更好地评估算法的准确度，比如我们用对象图块与输出图块区域的交集比上它们的并集，就可以得到一个可以评估准确度的参数，很明显，当两者完全重合时是最大值 1 ，否则都会比 1 小，我们称之为交并比（$loU$）

我们一般约定一个数来界定它是否识别准确，比如 0.5 ，那么当 $loU \geq 0.5$ 时认为检测正确，当然我们也可以更严格些，将这个参数设得更大，比如 0.7 这样

#### 4.3.7 Non-max suppression

非最大值抑制，听名字貌似能了解到它大概是在干嘛

![DeepLearning46](../img/Deep_Learning/DL46.png)

因为每个方块都有自己的输出，每个方块都可能认为自己这里有目标（或者说目标的中点），这就导致了同一个目标重复标记的情况

（下面以只检测车为例）

![DeepLearning45](../img/Deep_Learning/DL45.png)

我们可以这样去掉重复标记的目标，首先我们将交并比 $p_c$ （很明显我们将 $p_c$ 由 0 和 1 换成交并比会更灵活些）小于某个阈值的标记都去掉，因为那些都是没有识别出来的

然后我们从剩下的标记里面挑出来 $p_c$ 最高的那个，将其作为一个输出框框

其他那些还没有扔掉的，并不是最高的那些，我们将与已经确定输出的框框重叠度比较高的（交并比比较大，比如大于 0.5 这样）扔掉（这样就避免与已经输出的重复啦）

循环前两步，继续挑一个最大的，继续扔重叠比较高的，也就是每次都能输出一个框框，直到最后全部处理完成

#### 4.3.8 Anchor Boxes

![DeepLearning47](../img/Deep_Learning/DL47.png)

（这里仍然以 $3 \times 3$ 为例）

到现在我们还只能解决某个格子内出现一个对象的问题，如果一个格子内出现多个对象要怎么做呢？

![DeepLearning48](../img/Deep_Learning/DL48.png)

我们可以构造一些 $Anchor box$ ，这里为了方便就先使用 2 个，我们现在将各个对象匹配到最适合（根据交并比）它的 $Anchor box$ 下，比如前面那个重叠的格子内， $Anchor box 1$ 检测出来的是行人，而 $Anchor box 2$ 检测出来的是车辆，但要如何分配他们的输出呢？

我们知道我们原来每个格子输出 8 维的数据，现在我们每个格子的各个 $Anchor box$ 都输出一组数据，将它们数据列在一起，比如这里就是 16 维的数据

![DeepLearning49](../img/Deep_Learning/DL49.png)

这里上半部分（$Anchor box 1$）的输出就会是 $p_c = 1$，$b_x,b_y,b_h,b_w$ ， $c_1=1,c_2=0,c_3=0$ ，而下半部分（$Anchor box 2$）的输出就会是 $p_c = 1$，$b_x,b_y,b_h,b_w$ ， $c_1=0,c_2=1,c_3=0$

再比如我们的这张图片里只有车没有那个人（想象一下），那么下面的输出很明显是不变的，上面的 $p_c = 0$ ，其他的数值嘛，就可以不用关心了

这样我们就能在一个格子内检测出多个对象了，但是这种情况其实也并不是特别多见的，因为我们实际应用中会使用 $19 \times 19$ 这样子的分块，一个格子内多个对象的概率就很低了，但是我们使用这个方法有一个好处我们可以有针对性的检测出来某些形状的物体，比如说人就是很高很瘦的，车相对就是比较矮胖的

另外，这个算法很明显不能解决一个格子内存在高于 $Anchor box$ 数量个对象的情况，这种问题需要某些默认手段来处理，还有就是两个对象分配到同一个 $Anchor box$ 里，也是需要某些默认的手段来处理下

选择 $Anchor box$ 的方法的话，我们可以手工选择下（5-10 个），再或者 $YOLO$ 论文中提到的 k-平均算法，这样就可以自动选择 $Anchor box$ 啦

#### 4.3.9 Putting it together: YOLO algorithm

关于 $YOLO$ 算法的各种基础都已经讲过了，下面我们试着组装下

（以 $3 \times 3$ 划分、2 个$Anchor box$ 为例）

![DeepLearning50](../img/Deep_Learning/DL50.png)

如何构造训练集？貌似不用多说

![DeepLearning51](../img/Deep_Learning/DL51.png)

数据的输出也是这样的

![DeepLearning52](../img/Deep_Learning/DL52.png)

当然我们也可以使用非最大值抑制，当然每个格子都会输出自己的两个预测，但是某些“不自信”（小于某个交并比）的输出就自动“退出”了，之后循环运行非最大值抑制，最后就剩下我们需要的对象啦

#### 4.3.10 Region proposals

我们知道滑动窗口的方法会很慢（尚未使用一次卷积的方法的时候），需要对整张图片各个区域都进行遍历，那么我们能否找到一些区域，尽可能地略去那些空的背景区域以减少运算量呢

![DeepLearning53](../img/Deep_Learning/DL53.png)

我们可以使用某些算法从图片中获得色块，然后从色块中获取一些候选区域，之后卷积算法只在这些区域上跑就好了，这种算法称为 $R-CNN$

很明显，还是慢，还是每个都跑了，进一步改进自然就是使用对全图进行卷积类似的改进算法（类似于 4.3.4）

其实还是慢，之后又有一种改进是利用卷积神经网络获取候选区域

不过，$R-CNN$ 最终还是并没有 $YOLO$ 快……

不过它也的确是一个不错的想法，曾经对计算机视觉领域产生了很大的影响

### 4.4 Special applications: Face recognition &Neural style transfer

#### 4.4.1 What is face recognition?

很明显， Andrew Ng 在影片中演示的百度人脸门禁系统就是人脸识别的实现，现在已经可以做到实时比对了，并且可以做到活物检测，但是原理又是什么呢？

当然，我们这里的重点是人脸识别而不是活物检测，下面区分两个概念

- 人脸验证（face verification）

人脸验证要做的是，输入一张图片与其对应的姓名（或者 ID），那么该系统会输出对还是错，所以我们又称其为一对一的问题

- 人脸识别（face recognition）

人脸识别所要做的是，识别一张图片与一堆姓名的对应关系，所以又称一对多（$1:K$）问题

当我们有一个人脸验证系统的时候，比如它的准确率已经达到了 99% ，但如果我们用它来构建一个人脸识别系统的话，错误率会成倍增长，比如 $K = 100$ ，那么错误率会增长 100 倍，所以这就要求，如果我们利用人脸验证系统构建一个识别系统的话，验证系统的准确率需要足够的高，可能需要达到 99.9% 才够看

接下来，我们以人脸验证为基础构建一个人脸识别系统

#### 4.4.2 One-shot learning

人脸验证系统有个问题就是，我们不可能对每个人都采集很多数据来训练，我们只可能采集很少的一部分数据，这就是 $one-shot learning problem$

> 在我了解了神经网络的原理之后，我有时候会想手机的面部解锁是如何采集少量的图片就可以训练出来一个识别人的网络，很明显，这样的数据量不足以训练一个神经网络，但是我们很容易想到一个方法就是大量采集他人的数据，我们训练一个人脸比较的系统这样，不能训练直接地训练一张图片到底是不是该人，那么我们可以训练出来一个输出一张图片和另一张图片差异到底有多大的系统，下面的思路与此大同小异

以百度门禁为例，很明显，能投入实际应用的场景就是，我们采集每个员工一张图片，然后输出这张图片是不是我们的员工中的一人，如果按之前的思路，我们很自然地想到了使用 Softmax 神经网络，也就是 $(K+1)$ 分类（还有一个谁都没识别出来）问题

当然，上面的想法明显是不可行的，因为我们每个员工只采集了一张图片啊，怎么足以训练出来这样一个系统呢？

我们很容易想到了从其他数据上“迁移”过来，可是怎么迁移呢？

我们可以训练一个识别两张图片是否是一个人的系统，这样的话我们采集数据就会相对地容易些，很简单，我们训练一个函数，让它能够输出两张图片之间的差异值即可，当其大于某个阈值的时候我们认为这两张图片不是一个人

这样我们就很容易地做出来一个人脸验证系统，当然，这需要我们对数据库中的人逐个比对

#### 4.4.3 Siamese network

$Siamese Network$ 所完成的就是我们刚刚所说的那种验证方法

首先，我们输入一张图片，经过一系列卷积、全连接（不要 Softmax ，很明显我们现在没这个需求）操作最后我们会输出一个该图片的特征向量，当然其他图片也经过**这个**神经网络，这样每张图片都会有这么一个特征向量

我们可以认为这个向量很好的代表了这张图片，我们如何比对某两张图片呢？

我们可以利用这两个向量之差的范数来代表两张图片的差异值，比如图片 1 和图片 2 的差异值我们可以用 $||f(x^{(1)}) - f(x^{(2)})||^2$

这样，我们期待输入两个不同人的图片的时候会输出比较大的差异值，而输入两个相同人的图片的时候会输出比较小的差异值

所以我们要做的就是训练出来一个很好的神经网络，用来将图片编码成一个很好的特征向量

#### 4.4.4 Triplet loss

我们这里学习一种训练神经网络的方法，我们称之为 $Triplet$ 方法

由于我们最后要计算的两张图片的差异值，我们这里的一种想法是每组数据 ��� 使用三张图片，也就是三元组

![DeepLearning54](../img/Deep_Learning/DL54.png)

我们称上面左侧第一张图片是 Anchor ，第二张是 Positive ，Positive 和 Anchor 会是同一个人，而右侧第二张图片称为 Negative 它与 Anchor 是不同的两个人，我们简称这三张图片为 $A\ P\ N$

我们期望前两张图片的差异值小于后两张图片的差异值，也就是 $||f(A) - f(P)||^2 \leq ||f(A) - f(N)||^2$ ，当然，我们可以记距离函数 d 为 $d(A, N) = ||f(A) - f(P)||^2$

但这还是有点问题的，就是神经网络如果对任何图片都输出零向量，那么我们上面的等式会恒成立，这就使得神经网络一点东西都训练不到

为了防止这种情况的发生，我们可以让他们之间的差大一点，比如加一个超参数 $\alpha$ ，就像这样 $||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \alpha \leq 0$

下面我们定义一下损失函数，$L(A, P, N) = max(||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \alpha, 0)$ ，也就是上面的式子与 0 中的最大值，由于我们期望那个式子小于 0 ，那么如果它小于 0 ，损失值最小，是 0，当然我们并不需要关心它比 0 小多少，一旦小于就算对了，而一旦它大于 0 ，就说明出现了误差，这使得有了优化的方向（努力让损失降到 0 ）

我们刚刚计算的是某一组数据的损失值，整个数据集的损失的话，只需要求个和就好啦

另外，我们要注意下如何组成 $A\ P\ N$ 数据，$A$ 和 $P$ 随便找两张同一个人的图片就好，但是 $N$ 呢？我们随便再找一张的话，也许会很容易满足 $d(A, P) + \alpha \leq d(A, N)$ ，这样神经网络就学不到什么了，就类似于我们人类直观上感觉差异比较大的两个人是很容易分辨的

当然，我们应当尽量让其能分辨比较相似的两个 $A$ 和 $N$ ，也就是说 $d(A, P) \approx d(A, N)$ ，相当于对于人来说，直观上长得比较像的两个人，这就使得神经网络能学到更细致的算法

另外，想要做到这种程度的神经网络的话，必须要好多好多的图片，所幸的是，很多大公司将他们的数据集与预训练的神经网络参数，这就节省了我们大量的时间

#### 4.4.5 Face verification and binary classification

这里使用的是将人脸识别变成一个二分类方法，也就是，如果输入的两张图片是同一张图片的话，我们输出 1，而不同则输出 0

![DeepLearning55](../img/Deep_Learning/DL55.png)

我们将两张图片分别输入同一个神经网络，最后输出两组特征向量值，然后再将这两组向量输入一个神经网络，最后使用 logistics 获得 最终的输出

获得两个特征向量之后的处理需要特别说明下，比如我们使用这种方法 $\sigma (\sum \limits_{k=1}^{128} w_i|f(x^{(i)}) - f(x^{(j)})| + b)$ （假设输出的向量值是 128 维的），或者我们换成其他的，比如 $\sigma (\sum \limits_{k=1}^{128} w_i(f(x^{(i)}) - f(x^{(j)}))^2 + b)$

另外，由于我们在实际应用中的时候，每次都要提取数据库中员工的图片然后使用神经网络进行编码，但每次的编码过程是相同的，所以库内可以直接存储编码过的数据，而不需要存储原图片，这样可以大大提升识别速度

#### 4.4.6 What is neural style transfer?

![DeepLearning56](../img/Deep_Learning/DL56.png)

看图就知道啦

另外，我们记内容图为 $C$ ，风格图为 $S$， 生成图为 $G$

#### 4.4.7 What are deep ConvNets learning?

比如我们建立一个卷积神经网络，那么它每层分别都学到了些什么呢？

我们知道，我们每一个 filter 学到的东西最后会检测出来一个特征（参见 4.1.2），那么我们可以这样测试，单独取某个 filter 的输出值，然后看到底哪个图片或者图块会使其输出较大的激活值，那么这张图片或者图块很明显就是它所监测的特征了

首先我们从第一层取出 9 个 filter ，看其激活值较高的图块或者图片是什么

![DeepLearning57](../img/Deep_Learning/DL57.png)

首先我们看第一层，我们能看到，左上角的 filter 检测出来的特征大概类似于右面的这种斜线特征，第 2 、3 个也检测出来类似地特征，第 4 个大概检测出来了橙色，其他的大概都是形状、边缘、颜色等等的特征

![DeepLearning58](../img/Deep_Learning/DL58.png)

我们再看第二层，很容易发现，第二层检测的特征已经复杂的多了，比如第二个就是检测一条一条这种，第六个就是检测左面有点圆形这种

![DeepLearning59](../img/Deep_Learning/DL59.png)

第三层，可检测的特征更加复杂了，第一个可以检测格子状的东西，第五个已经可以检测轮胎这样的了

![DeepLearning60](../img/Deep_Learning/DL60.png)

第四层，第一个已经可以检测出来狗了，很吃惊吧，只不过这里的狗貌似都差不多，第三个大概是水，第六个大概是细长的鸟腿

![DeepLearning61](../img/Deep_Learning/DL61.png)

第五层，已经可以检测出来更多种的图片了……

这就是对神经网络各层所做的事的一个可视化，有点意外，但是这样也能解释神经网络的迁移是可以实现的了，这样的可视化确实对某些网络构建方法的理解很有帮助

#### 4.4.8 Cost function

我们如何定义代价函数呢？

$J(G) = \alpha J_{content}(C, G) + \beta J_{style}(S, G)$

这里代价函数分为了两部分，一部分是 $C$ 与 $G$ 之间的，另一部分是 $S$ 与 $G$ 之间的，另外增加了两个超参数 $\alpha$ 和 $\beta$ （只用一个超参数应该就够了，只是为了让它们不是直接叠加而已，这里与原作者论文保持一致，用两个）

我们要做的就是首先随机生成一张图片（比如 $100 \times 100 \times 3$），然后使用梯度递降，$G = G - \frac{\partial}{\partial G} J(G)$

![DeepLearning62](../img/Deep_Learning/DL62.png)

上面就是对图片的优化过程了，前两张是 $C$ 和 $S$ ，第三张是初始化的白噪声图，后面是不断优化生成的图片

#### 4.4.9 Content cost function

这里我们定义下 $J_{content}(C, G)$

如何定义内容的相似性呢？我们由 4.4.7 知道了神经网络各层分别检测了各种层面的对象，如果我们使用低层进行评估的话，那么“相似”的图片会是像素层级的相似，而比较深的层的话，比如这层能够识别狗了，而原图有狗的话，他就会确保生成的图片中会有狗

所以呢，我们会**选择不会太深也不会太浅的层，也就是中间层来评估内容上的相似性**

首先我们选取一个已经预训练好的神经网络，选取其中间某层 $l$ ，然后让 $C$ 与 $G$ 同时通过该网络，以隐藏层 $l$ 的输出为最终输出

定义 $J_{content}(C, G) = \frac{1}{2} ||a^{[l](C)} - a^{[l](G)}||^2$ ，原论文中使用了归一化，但是没有归一化大抵上是没太大关系的，因为我们有着一个超参数来调整

#### 4.4.10 Style cost function

这里我们定义下 $J_{style}(S, G)$

那么什么是图片的风格呢？

![DeepLearning63](../img/Deep_Learning/DL63.png)

我们这里取神经网络某个隐藏层进行分析，我们知道，每个通道是一个过滤器的输出，而每种过滤器代表了某种检测对象，所以每个通道就对应了原图中各个区域内是否有该特征

为了方便，不妨认为通道一检测是否是橙色，通道二检测是否有竖线

我们取第一个通道右下角的这个值，它代表了原图右下角是不是橙色，而第二个通道对应位置的值代表了原图该位置有没有竖线，这两个特征有什么关联呢？我们不妨认为风格就是两种特征之间的相关性，比如说图上大多数出现橙色的地方也出现了竖线，这就是相关性比较大的体现，否则相关性较小

刚刚我们已经大概地定义了相关性的计算方法，我们现在详细地讨论下

> 我们知道，两个独立变量 $E(XY) = E(X)E(Y)$ ，而协方差 $E(XY) - E(X)E(Y)$ 越大，$X$ 与 $Y$ 相关性越大

> 我们使用 $a_{i, j, k}^{[l]}$ 表示在 $i, j, k$ 位置的激活项，$i$ 对应输出高度号，$j$ 对应输出宽度号， $k$ 对应输出的通道号

首先，我们对比的是两个相同位置的相关性，也就是两者同时出现的概率，我们使用两个激活值相乘试试（$XY$ 嘛，统计整个图像之后就正比于 $E(XY)$ 啦，减去 $E(X)E(Y)$ 先不考虑），我们将每两通道之间的结果放在一个位置，也就是 $G_{kk'}^{[l]} = \sum \limits_{i=1}^{n_H^{[l]}} \sum \limits_{j=1}^{n_W^{[l]}} a_{i, j, k}^{[l]} a_{i, j, k'}^{[l]}$ ，也就是我们最后得到了 $n_c \times n_c$ 的矩阵，每个位置表示了某两种特征之间的相关性

这样我们就能分别得到 $S$ 和 $G$ 的风格了，分别是

- $G_{kk'}^{[l](S)} = \sum \limits_{i=1}^{n_H^{[l]}} \sum \limits_{j=1}^{n_W^{[l]}} a_{i, j, k}^{[l](S)} a_{i, j, k'}^{[l](S)}$
- $G_{kk'}^{[l](G)} = \sum \limits_{i=1}^{n_H^{[l]}} \sum \limits_{j=1}^{n_W^{[l]}} a_{i, j, k}^{[l](G)} a_{i, j, k'}^{[l](G)}$

然后我们比较这两个风格矩阵就好啦

各层的（是否归一化影响不大）：

$J_{style}^{[l]}(S, G) = \frac{1}{(2n_H^{[l]} n_W^{[l]} n_C^{[l]})^2} \sum\limits_k \sum\limits_{k'}(G_{kk'}^{[l](S)} - G_{kk'}^{[l](S)})$

各层相加，让其在每个层次都有相似度（可调整超参数 $\lambda^{[l]}$）

$J_{style}(S, G) = \sum \limits_l \lambda^{[l]} J_{style}^{[l]}(S, G)$

终于将内容代价与风格代价都弄出来了，最后加在一起啦

$J(G) = \alpha J_{content}(C, G) + \beta J_{style}(S, G)$

#### 4.4.11 1D and 3D generalizations of models

一维、三维卷积操作和二维的一毛一样……

## 5 Sequence Models

::: tip 第五章哪里去了？

由于第五章在编译时会出现错误，且该章节内容暂时很少用到，因此暂时注释掉了

:::

<!--

### 5.1 Recurrent Neural Networks

#### 5.1.1 Why Sequence Models?

一些例子

- 语音识别
- 音乐生成 输入数据甚至可以是空
- 情感分类
- DNA 序列分析
- 机器翻译
- 视频行为识别
- 命名实体识别 从句子中识别出人名

#### 5.1.2 Notation

比如我们想从这句话里识别出来人名

Harry Potter and Hermione Granger invented a new spell.

我们可以记 $x^{<t>}$ 输入数据的第 $t$ 个单词，比如说这里的 $x^{<3>}$ 就是 and

可以记 $T_x$ 为输入数据的总词汇量，$t$ 是指在时域的推进

我们知道，$x^{(i)}$ 表示第 $i$ 组输入数据，那么 $x^{(i)<t>}$ 就是第 $i$ 组数据的第 $t$ 个单词，$T_x^{(i)}$ 就表示该语句单词的个数（不同语句单词数可能不一样）

然后怎么来表示呢？

![DeepLearning64](../img/Deep_Learning/DL64.png)

我们建立一个词汇表，将所有单词罗列出来，比如如图上所示 $a$ 是第 1 个，$and$ 是第 367 个等等

我们以此表示语句，比如说我们这句话第一个单词就应表示为除了第 4075 行是 1 其余都是 0 的列向量，我们叫这种只有一个是 1 ，其余全是 0 的向量为 one-hot 向量

但当有词汇表里没有的词汇出现怎么办？我们可以制定一个 Unknown word ，记为 `<UNK>`

#### 5.1.3 Recurrent Neural Network Model

就上面的例子而言，如果我们使用传统的神经网络的话，输入向量维度不同、参数过多，而且学习到的东西并不共享

![DeepLearning65](../img/Deep_Learning/DL65.png)

循环神经网络是逐步对每个词汇扫描，第一个时间步，第一个单词经过神经网络输出 $\hat{y}^{<1>}$ ，并且其激活值 $a^{<1>}$ 也会参与下一层的运算，这就使得每个时间步都会向后传递

当然，为了保持一致，我们需要设置一个 $a^{<0>}$ ，通常将其设置为 0 向量

本模型的一个问题就是，虽然每个时间步都会向后传递，但是并不会向前传递

我们记 $W_{aa}$ 为处理上层激活值的参数，$W_{ax}$ 为处理输入的参数， $W_{ay}$ 为处理本层激活值获得输出的参数

所以

- $a^{<t>} = g_1(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a)$
- $\hat{y}^{<t>} = g_2(W_{ya}a^{<t>} + b_y)$

这里 $g_1$ 常用 tanh ，有时也用 ReLU， $g_2$ 看情况，比如这里识别人名的话，会选用 sigmoid

我们可以再简化下，记

$$
W_a = \\
\begin{bmatrix}
W_{aa} & W_{ax} \\
\end{bmatrix}
$$

那么很明显，我们的第一个式子可以写成

$$
a^{<t>} = g_1(W_a
\begin{bmatrix}
a^{<t-1>} \\
x^{<t>} \\
\end{bmatrix} + b_a)
$$

第二个式子也可以简写为

$\hat{y}^{<t>} = g_2(W_{y}a^{<t>} + b_y)$

#### 5.1.4 Backpropagation through time

和传统神经网络一样，反向传播就是逆着前向传播逐渐进行的

![DeepLearning66](../img/Deep_Learning/DL66.png)

![DeepLearning67](../img/Deep_Learning/DL67.png)

#### 5.1.5 Different types of RNNs

我们在 5.1.1 中提到了很多的应用，但是很明显，大多数并不适用前面的模型，因为他们的 $T_x \not= T_y$，这需要我们适当修改下

![DeepLearning68](../img/Deep_Learning/DL68.png)

- 一对一的问题我们以前就可以解决了
- 一对多，比如音乐的生成，我们输入一个整数代表音乐的类型或者什么都不输入，它会为我们生成一系列音符序列，很明显，后面的时间布并没有输入，不过我们通常会将前一层的输出喂给下一层
- 多对一，比如从一段话里识别一个人对电影的评价（1 到 5 星），那么我们可以只在最后一层输出，所以它会综合前面所有单词输出一个评价
- 多对多 $T_x = T_y$ ，就是我们刚刚的人名识别类型
- 多对多 $T_x \not= T_y$ ，语言翻译，前几时间步仅做输入（称为 encoder），后几个时间步仅做输出（称为 decoder）

#### 5.1.6 Language model and sequence generation

什么是语言模型？

比如我们在做一个语音识别系统，有这样一个句子

"the apple and pear(pair) salad was delicious."

但是到底是 "the apple and pair salad"，还是 "the apple and pear salad ？"

一个语音识别模型可以根据前面的 "the apple and" 计算出 后面词是 "pair" 的概率和 "pear" 的概率

（有点像搜索候选）

在建立模型之前，我们需要一个语料库作为训练数据，另外我们要用 `<UNK>` 标记不清楚的单词，用 `<EOS>` 标记句子结尾

这里以 "Cats average 15 hours of sleep a day." 这句话为例

![DeepLearning69](../img/Deep_Learning/DL69.png)

首先，我们从一个零向量开始，预测第一个词，所以第 0 个时间步输入 $x^{<1>} = 0$ ，其输出期待为 $y^{<1>}$ 也就是 "cats"（用 Softmax 就好），相应地，它会输出各个词的概率，可能这里 "cat" 的概率会很高

然后，我们下一个时间步以 $x^{<2>} = y^{<1>}$ 也就是 "cats" 作为输入，其输出期待为 $y^{<2>}$ "average"，这里也会输出各个词语的概率，可能这里 "average" 概率很高

再下一个时间步，以 $x^{<3>} = y^{<2>}$ 也就是 "average" 作为输入，当然，我们这里同时也考虑到了前面的激活值，所以可以认为是考虑到输入 "cats average" 的情况下，我们期待其输出 "15"

以此类推 ...

我们看下代价函数

$L(\hat{y}^{<t>}, y^{<t>}) = - \sum_i y_i^{<t>} \log \hat{y}_i^{<t>}$

总体的损失函数自然就是相加啦

$L = \sum_t L^{<t>}(\hat{y}^{<t>}, y^{<t>})$

比如我们有这么一个句子 $y^{<1>}, y^{<2>}, y^{<3>}$

- 第零个时间步它会输出 $y^{<1>}$ 的概率，也就是 $P(y^{<1>})$
- 第一个时间步它会输出在考虑 $y^{<1>}$ 的情况下 $y^{<2>}$ 的概率，也就是 $P(y^{<2>}|y^{<1>})$
- 第三个时间步它会输出在考虑 $y^{<1>}, y^{<2>}$ 的情况下，$y^{<3>}$ 的概率，也就是 $P(y^{<3>}|y^{<1>}, y^{<2>})$

很明显，输出这个句子的概率会是 $P(y^{<1>}, y^{<2>}, y^{<3>}) = P(y^{<1>}) P(y^{<2>}|y^{<1>}) P(y^{<3>}|y^{<1>}, y^{<2>})$ ，也就是他们三项相乘

#### 5.1.7 Sampling novel sequences

我们如何看这个模型到底都学到了些什么呢？

首先，我们令 $a^{<0>} = 0$ , $x^{<1>} = 0$ ，然后 $y^{<1>}$ 便会从 Softmax 中输出每个单词的概率，我们可以使用 $np.random.choice$ 来根据概率采样，比如说我们第一个词采样为 'the'

下面我们将 'the' 喂给 $x^{<2>}$ ，再根据概率采样下一个单词，以此类推，一直到 `<EOS>` 标志出现，最终我们会输出该模型生成的句子

另外，有可能生成含 `<UNK>` 的句子，那么我们就重新采样就好了

除了按照单词建立词汇表，还可以按照字符建立词汇表，这样的好处就是不会出现 `<UNK>` ，但是它训练起来的代价更大些

#### 5.1.8 Vanishing gradients with RNNs

很明显，RNN 并不擅长捕捉很久之前出现的词汇，每一个输出仅对应前几个的输入，比较远的输入对该输出的影响会比较小，所以，下面会着重介绍该问题的解决办法

#### 5.1.9 Gated Recurrent Unit (GRU)

我们知道，原来我们在不同时间步之间传值是使用激活值 $a^{<t>}$ ，很明显，这样很容易发生梯度消失的情况，而门控循环单元 GRU 比较好的解决了该问题

这里我们不使用 $a^{<t>}$ 传递时间步之间的值，而是使用记忆细胞 （memory cell） $c^{<t>}$ 进行传递，或者说 $a^{<t>} = c^{<t>}$

$$
\begin{aligned}
\tilde{c}^{<t>} =& \tanh (W_c [c^{<t-1>}, x^{<t>}] + b_c) \\
\Gamma_u =& \sigma(W_u [c^{<t-1>}, x^{<t>}] + b_u) \\
c^{<t>} =& \Gamma_u * \tilde{c}^{<t>} + (1-\Gamma_u) * c^{<t-1>}
\end{aligned}
$$

首先根据该时间步计算候选值 $\tilde{c}^{<t>}$ ，之后使用 sigmoid 计算一个更新门，很明显，它的值会接近于 1 或者接近于 0 ，然后我们利用这个更新门看到底是否要更新 $c^{<t>}$ ，由于它很接近于 0 或 1 ，所以该值只有两种可能，即传递和未传递，所以不会发生梯度消失的现象

当然，我们不可能只传递某一个特征所以这就要求我们的 $c^{<t>}$ 必须是某个维度的向量，比如 100 维的，那么 $\tilde{c}^{<t>}$ 和 $\Gamma _u$ 也必然是 100 维的

完整的 GRU 在第一个式子上额外做了“手脚”

$$
\begin{aligned}
\tilde{c}^{<t>} =& \tanh (W_c [\Gamma_r * c^{<t-1>}, x^{<t>}] + b_c) \\
\Gamma_u =& \sigma (W_u [c^{<t-1>}, x^{<t>}] + b_u) \\
\Gamma_r =& \sigma (W_r [c^{<t-1>}, x^{<t>}] + b_r) \\
c^{<t>} =& \Gamma_u * \tilde{c}^{<t>} + (1-\Gamma_u) * c^{<t-1>}
\end{aligned}
$$

这里额外增加的 $\Gamma_r$ 说明了计算出的下一个 $c^{<t>}$ 的候选值 $\tilde{c}^{<t>}$ 跟 $c^{<t-1>}$ 有多大的相关性

#### 5.1.10 LSTM (long short term memory) unit

![DeepLearning70](../img/Deep_Learning/DL70.png)

现在我们使用了三个门，更新门、遗忘门、输出门，另外， $c^{<t>}$ 也不等于 $a^{<t>}$ 了，他们各占据一条线进行传递

另外 LSTM 有时候会令上一个记忆细胞的值 $c^{<t-1>}$ 参与门值的运算，我们又称之为偷窥孔连接

LSTM 是比 GRU 更早的模型，它比 GRU 更复杂也更灵活，GRU 是对其进行了简化的版本

#### 5.1.11 Bidirectional RNN

我们知道，传统 RNN 只构建了单向的传播路径，并不会考虑后面的几个单词，所以我们就需要构建双向的 BRNN

![DeepLearning71](../img/Deep_Learning/DL71.png)

和 RNN 相比，只不过多了一个反向的连接，我们现在 $\hat{y}^{<t>} = g(W_y [\overrightarrow{a}^{<t>}, \overleftarrow{a}^{<t>}] + b_y)$

另外，BRNN 完全可以建立在 LSTM 和 GRU 之上的

#### 5.1.12 Deep RNNs

![DeepLearning72](../img/Deep_Learning/DL72.png)

将每个单元都变成深层，且每一层都与下一个时间步相连接，但是往往这样的连接层只有 3 层，因为有着时间的维度，RNN 会变得相当的大，不过我们可以在输出前建立几层，只不过不在时间步上建立连接

### 5.2 Natural Language Processing and Word Embeddings

#### 5.2.1 Word Representation

我们一直使用 one-hot 向量来表示一个词汇，但是这样显然是不太好用的，因为任何两个词之间的距离都是一样的，比如说，orange 和 apple 与 orange 和 king 之间的距离是一样的，但是我们知道，apple 和 orange 之间的距离应该更小些，因为它们有着很多的相似之处，如何做到这一点呢？

![DeepLearning73](../img/Deep_Learning/DL73.png)

我们可以使用一个向量来表示一个词，比如，第一个维度表示他的性别，第二个维度表示他有多高贵等等……（比如说我们弄了 300 个维度）

很明显，使用这种表示方法的话，orange 和 apple 之间的距离就会相对小些

我们将这样 300 维度的数据使用 t-SNE 算法在二维空间表示出来，就会有下面这样的结果

![DeepLearning74](../img/Deep_Learning/DL74.png)

这样，各个词语之间的关系一目了然，每个词像嵌入在 300 维空间的某个位置，故称其为词嵌入

#### 5.2.2 Using Word Embeddings

如何使用词嵌入呢？这里仍然以命名实体进行举例

首先看普通的 one-hot 向量，比如说这样一个句子： "Sally Johnson is an orange farmer." ，因为我们知道 orange farmer 是人，所以 Sally Johnson 一定就是人名了，再看一个句子 “Robert Lin is a durian cultivator.”，可是我们的词汇表里没这两个词怎么办，这样就没办法了呀

但如果我们使用从其他训练集里训练的词嵌入的话，它很容易知道 durian 和 orange 很相似，cultivator 和 farmer 很相似，这样就很容易训练我们的命名实体模型啦

值得注意的是，我们之所以在这里使用这样的方法是因为我们命名实体的训练集一般比较小，所以我们会考虑这样类似于迁移学习的方法，但如果是语言模型或者机器翻译模型的话，我们一般不太会使用这种方法，因为他们已经有着很多很多的数据了

另外，词嵌入和人脸编码有着点联系，这里的人脸编码是指训练出将人脸转化为特征向量的神经网络，其输出的向量有点类似词嵌入的效果，是将各个编码嵌入到不同的位置……

#### 5.2.3 Properties of Word Embeddings

如果 man 对应 woman 的话，那么 king 对应什么？为什么？

很明显 king 对应 queue ，我们可以知道 $e_{man} - e_{woman} \approx e_{king} - e_{queue}$ 所以我们说 king 对应 queue

所以我们就相当于找一个词 $e_w$ ，使其与 $e_{king} - e_{man} + e_{woman}$ 相似，我们定义他们之间的相似度 $sim(e_w, e_{king} - e_{man} + e_{woman})$

我们通常使用余弦相似度来表示他们之间的相似度

$sim(u, v) = \frac{u^T v}{||u||_2||v||_2}$

或者使用平方距离或者欧式距离来表示

$||u-v||^2$

不过我们更常用的还是余弦相似度

#### 5.2.4 Embedding Matrix

我们的嵌入矩阵 $E$ 应该是这样的， 300 行，每行一个特征， 10000 列，每列一个词语，这样，当我们使用 $E \cdot o_j$ （$o_j$ 表示词语 $j$ 的 one-hot 向量）得到的当然是这个词语的特征向量啦

不过，我们实际操作的时候不会这样乘，因为这样会浪费大量运算（与 0 相乘）

#### 5.2.5 Learning Word Embeddings

那么我们要如何学习这样一个词嵌入的矩阵呢？

我们有一个句子 "I want a glass of orange \_\_\_\_"

![DeepLearning75](../img/Deep_Learning/DL75.png)

我们通过嵌入矩阵 E 将他们转化为特征向量，然后我们将他们喂入一个 Softmax 层，用来预测最后一个词的 one-hot 向量

在训练的过程中，嵌入矩阵 E 是被不断优化的，最终就会得到我们想要的嵌入矩阵

另外，我们并不是将整个句子都喂入神经网络，通常我们只喂入一部分，称其为 context ，context 的选定就是个超参数了，比如说选定前面四个、前后各四个等等，更好的选定方法见下一节

#### 5.2.6 Word2Vec

下面我们了解下 Skip-Gram 模型

要怎么选取 context 呢？我们可以随机从前后几个词的范围内取出一个词，作为其预测结果

然后就，训练咯，但是要注意的一点就是，普通 Softmax 的计算可是要对每个单词都计算求和的，这就使得在求这一步的时候速度会很慢，因为我们的词典可能上万甚至上百万，所以我们可以使用另一种方法优化下

比如说，多级 Softmax，每级都是 logistics，以 10000 个词为例，比如我们预测的是 3800 这个位置，第一级会预测其在前 5000，第二次会预测其在 2500 到 5000，第三次会预测其在 3750 到 5000 …………………………

这样的计算量就会大大减少，最终也只是词典量的对数次

很明显，这样的话，我们会构建出来一棵树，但是一般情况下我们不会使用平衡二叉树，而是会根据各词汇的权重构建出来的，这会使得出现频率较高的会更容易的查找到

另外，我们取 context 的时候如果是均匀采样的话，那会发现， the、of、a、and、to 出现的频率会非常之高，那么我们训练出来的模型会大量预测这些词，很明显我们并不想要这样的结果，所以我们采样的时候也会按照权重来采

#### 5.2.7 Negative Sampling

这是另一种学习词嵌入的方法

我们像 Skip-Gram 一样从句子中选取目标附近的一个词作为正样本，比如，以 orange 为目标，选取句子中的 juice ，那么这对词 orange / juice 将作为一对正样本，他们对应的 y 是 1

然后我们再从词典中随便选取 k 个词，将 orange 和他们作为负样本，比如 orange 和 king、orange 和 book、orange 和 the、orange 和 of，他们对应的 y 都是 0

然后拿他们进行训练就行啦

另外就是到底以什么频率采样，一个经验算法是 $P(w_i) = \frac{f(w_i)^{\frac{3}{4}}}{\sum \limits_{j=1}^{10,000}f(w_j)^{\frac{3}{4}}}$

#### 5.2.8 GloVe Word Vectors

这是另一种算法，相对于前两种算法简单些

我们令 $X_{ij}$ 为 i 出现在 j context 的频率，当然，如果我们取 context 为前后 10 个单词这样的话，$X_{ij} = X_{ji}$

我们最小化 $\sum \limits_{i=1} ^{10,000} \sum \limits_{j=1} ^{10,000} f(X_{ij}) (\theta_i^T e_j + b_i + b_j' - \log X_{ij})^2$ 就可以了

#### 5.2.9 Sentiment Classification

一般情况下，我们的情感分类问题的数据集可能比较小，但是我们如果有了词嵌入的话，也是可以很好地建立这样一个模型的

用 RNN 就好，最后用 Softmax 输出一个“星级”

#### 5.2.10 Debiasing Word Embeddings

因为 AI 越来越多地应用到正式场合了，所以有些问题我们不得不去考虑，比如我们训练的词嵌入中可能会有些偏见的，比如性别偏见、种族偏见

这里以性别偏见为例，我们如何来消除这种偏见呢？

1. 首先我们找到那些在性别上真的有区别的词对，比如 he - she 、male - female，对他们取平均，我们会获得一条无性别偏见的中间的线

2. 然后我们中和，将某些出现性别“偏见”的词语中和到无偏见线上

3. 最后均衡，将某些不对称的词对移动到无偏见线的相应的对称位置，防止因为不对称而使得某个词语与他们之间的距离不一致

### 5.3 Sequence models & Attention mechanism

#### 5.3.1 Basic Models

本周的主要内容是 seq2seq 模型，首先从机器翻译开始吧

我们之前了解过，我们会将源语言输入 encoder ，这时候会产生一个特征向量，然后我们再经过一个 decoder 就可以依次输出序列啦，然后直到遇到结束的标记，另外一个值得注意的就是，decoder 这里，我们会将前一时间步的输出放到下一时间步的输入

另外，我们还可以让 RNN 实现图片描述，当然，这是需要 CNN 的，一张图片经过 CNN 卷积得到特征向量，传入 RNN decoder 就完成啦

#### 5.3.2 Picking the most likely sentence

![DeepLearning76](../img/Deep_Learning/DL76.png)

seq2seq 和前面的语言模型相比，其实就多了个 encoder 的初始值嘛，但是细节上也是有些区别的

我们如何选取最可能的翻译句子呢？很明显，我们应该选择最大概率的那个，但是我们应该怎么寻找概率最大的那个呢，因为我们知道，一句话的概率是要每个输出的概率相乘才能得出的，这绝对不是每一步都选取最大的这样简单，但要将整个单词空间（10000）都探测一遍（如果生成 10 个词语，有 $10000^10$ 种可能），就太费时间了，所以我们要设计比较好的搜索算法

#### 5.3.3 Beam Search

有一种比较常用的搜索方法是集束搜索，下面以集束宽为 3 进行举例

首先 encoder 输出一个特征向量，decoder 接收后经过 Softmax 得到一系列第一个词的概率分布（a 到 zulu）

然后我们从中挑选三个概率最高的，比如 in 、 jane 、 september

分别根据这三个词得到下一个词的分布，很明显，每个词 10000 个分布，3 个词共有 30000 个分布，我们再从中选出 3 个概率最高的，比如 in september 、 jane is、 jane visits 这样前两个词就只有三个了，以此类推

这样下来，我们每次都是从 30000 个里面选出 3 个（除了第一次），这样我们可以大大的减少原来的分布数，而且会得到比较好的结果

#### 5.3.4 Refinements to Beam Search

我们一直在做的就是

$arg max \prod\limits_{t=1}^{T_y} P(y^{<t>} |x, y^{<1>}, \cdots, y^{<t-1>})$

嘿，看着有点吓人，但是实际上就是每个输出词在前提条件下的分布概率啦，简单地说，就是各步概率的乘积

但是这有一个问题就是，我们很可能最后得到一个很小很小的数，以至于下溢（浮点数）

为避免这种问题，我们将它改成

$arg max \sum\limits_{t=1}^{T_y} \log P(y^{<t>} |x, y^{<1>}, \cdots, y^{<t-1>})$

这样就可以避免这个问题啦

另外，因为我们长度每增加一个概率就会减小，所以我们的算法可能会更倾向于输出比较短的句子，所以我们需要对长度进行归一化

$arg max \frac{1}{T_y^\alpha}\sum\limits_{t=1}^{T_y} \log P(y^{<t>} |x, y^{<1>}, \cdots, y^{<t-1>})$

这里的 $\alpha$ 常常会取 0.7 ，这样就可以对长度进行一定的归一化处理啦

另外，我们集束搜索的集束宽 B 要怎么设定呢，很明显，如果 B 比较大，可能会更准确，但是可能会更慢，下面我们会讨论下如何更好地选取 B

> 集束搜索与 BFS、DFS 不同的是，它并不是一种精确搜索方法，因为它不能对全空间进行搜索，只能得到一个相对精确的答案

#### 5.3.5 Error analysis in beam search

如果我们一个模型出了问题，如何判定它到底是集束搜索出了问题还是 RNN 出了问题呢？

我们有一个人工翻译的句子 $y^*$，当然，它会比输出错误的句子 $\hat{y}$ 更好，但是我们不知道到底是因为 RNN 已经认为 $y^*$ 更好，但是集数宽太小没有搜索到，还是因为 RNN 就是认为 $\hat{y}$ 比 $y^*$ 更好呢？

那么我们就看下 RNN 对 $y^*$ 和 $\hat{y}$ 的态度吧

分别将 $y^*$ 和 $\hat{y}$ 输入 RNN ，对比 $P(y^*|x)$ 和 $P(\hat{y}|x)$ ，如果 $P(y^*|x)$ 更大，就说明 RNN 的选择是正确的，集束搜索出了问题，反之则是 RNN 出了问题

> 注意，如果用了长度归一化，那么这里比较概率的时候也应按照长度归一化的计算方法来处理

我们可以利用开发集找出各个产生错误的数据，统计他们问题出现在集束搜索上的比例和出现在 RNN 上的比例

如果集束搜索上的问题比较大，我们会增大集束宽，反之会考虑 RNN 上出现了什么问题，是需要正则化还是增大数据集等等

#### 5.3.6 Bleu Score

我们如何评估我们一个模型的效果呢？我们已经知道了单一实数评估的重要性，可是我们机器翻译以及图片描述这种往往有着很多种比较好的答案，那要如何得到这样一个单一实数呢？

对于一个语句，我们可能有着多个最佳的翻译结果（人工），我们将他们都作为参考结果

对于机器翻译的某一个结果，我们如何评估呢？如果我们单纯的评估各个单词出现在参考里面的频率，结果可能很差，比如机器翻译出来了 "the the the the the the the" 但是参考里面确实有 "the" ，那么他的得分就会是 $\frac{7}{7}$ 了，我们可以根据参考结果给每个词设上限值，比如第一个参考结果出现了两个 the ，第二个参考结果出现了一个 the ，那么 the 的上限就是 2，所以我们的得分只能是 $\frac{2}{7}$ ，我们将这种只根据一个单词的评估结果记为 $P_1$

然后我们使用两个词进行评估，看各个二元词组在参考结果中的出现频率如何，记为 $P_2$

此后还有 $P_3$ 、 $P_4$

然后对 $P_1, P_2, P_3, P_4$ 求和并取平均，然后指数化，再考虑一下长度惩罚，就得到了 Bleu 得分，这为我们机器翻译、图片描述的提供了单一实数的评估方法

#### 5.3.7 Attention Model Intuition

我们人类在翻译语句的时候是先从源语句的前一部分翻译得到语句的前一部分，然后接着向后一点点推移进行翻译的，而我们前面所使用的 RNN 是将整个语句经过 encoder 压缩成一个向量后传递到 decoder ，这样固然不错，但是在词汇量比较多的时候， Blue 得分会有很明显的下降，我们要如何解决呢

下面我们简单了解下注意力模型

![DeepLearning77](../img/Deep_Learning/DL77.png)

我们是将源语句经过双向 RNN 输出一些结果，然后将他们的结果通过权重链接到输出的 RNN 上，比如 $\alpha^{<1,1>}$ 代表源语句第一个词对输出结果第一个词的影响会有多大

#### 5.3.8 Attention Model

我们现在就要考虑下怎么学习这个注意力权重啦，我们可以搭建一个小型的神经网络，用来学习某两个时间步之间注意力权重

#### 5.3.9 Speech recognition

我们如何通过一个 seq2seq 模型构建一个语音识别模型呢？

![DeepLearning78](../img/Deep_Learning/DL78.png)

其中一种方法就是使用前面提到的注意力模型

![DeepLearning79](../img/Deep_Learning/DL79.png)

还有一种称为 CTC 损失函数的方法，它是通过允许输出一个被扩增的文本段实现的，原来 19 个字符被扩增到和输入序列等长的 1000 字符

#### 5.3.10 Trigger Word Detection

如何训练一个语音助手对触发字作出响应？

![DeepLearning80](../img/Deep_Learning/DL80.png)

我们可以在这样一个语音序列上有触发字的位置标记为 1 ，其余位置标记为 0 ，然后训练就好啦

#### 5.3.11 A New Beginning

$Thank\ you,  Andrew\ Ng$

 -->

# References

1. [深度学习工程师 - deeplearning.ai - 网易云课堂](https://mooc.study.163.com/smartSpec/detail/1001319001.htm)
2. [机器学习初学者](http://www.ai-start.com/)
3. 《Python 神经网络编程》 Tariq Rashid
4. [2017CS231n 斯坦福李飞飞视觉识别 - 网易云课堂](https://study.163.com/course/courseMain.htm?courseId=1004697005)
