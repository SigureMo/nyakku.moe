{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return (abs(x) + x) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN 的输入\n",
    "\n",
    "与传统网络不同的是，$GCN$ 的输入并不只是的特征，它额外包含了图中各个结点之间的关系，那么关系用什么表征呢？显而易见，邻接矩阵是一个非常简单易行的表示方法，这里**使用邻接矩阵 $A$ 来表示各个顶点之间的关系**，显然 $A.shape = (N, N)$\n",
    "\n",
    "相应地，各个顶点的特征使用矩阵组织起来就好啦，这里使用 $X$ 表示输入层特征矩阵， $H^i$ 表示第 $i$ 隐藏层特征矩阵，当然 $X = H^0$ ，显然 $H^i.shape = (N, F^i)$\n",
    "\n",
    "> $F^i$ 表示第 $i$ 层的特征维度，$N$ 表示顶点个数\n",
    "\n",
    "### 与 CNN 的相似性\n",
    "\n",
    "我们知道，普通 $CNN$ 的卷积是同时对空间与通道进行卷积，而 $Xception$ 提出的深度可分离卷积是对空间卷积与通道的全连接进行的分离，而且深度可分离卷积确实比普通卷积有着更少的参数，在等量参数下，深度可分离卷积有着更好的效果\n",
    "\n",
    "深度可分离卷积将一个卷积层分成两个子层，第一步是对每个通道进行空间上的卷积，各个通道独立操作，第二步是对通道进行线性组合，也就是传统神经网络所做的事，那么对于一个图数据，是否也可以这样做呢？\n",
    "\n",
    "假如一张图也是可以卷积的，那么我们首先对它进行卷积，之后对特征进行全连接，这便是 $GCN$ 的基本结构了\n",
    "\n",
    "相应的，特征的维度便是传统网络的某一层结点单元数，特征维度的变换便是各层结点之间的全连接，当然这也说明了为何 $W^i.shape = (F^i, F^{i+1})$ ，当然，$W^i$ 的所有参数都用在了两层之间全连接上了，图卷积并无参数（区别于 $CNN$ 卷积核需要参数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图卷积网络层的构建\n",
    "\n",
    "首先考虑一种非常非常简单的传播方式\n",
    "\n",
    "$$\n",
    "H^{i+1} = g(A H^i W^i)\n",
    "$$\n",
    "\n",
    "> $g$ 为激活函数，$W^i$ 为第 $i$ 层的参数，$W^i.shape = (F^i, F^{i+1})$，另外值得注意的是，这里的乘法均为矩阵乘法，仅仅看 $shape$ 便可以知道这种变幻的合理性了， $(N, N) \\cdot (N, F^i) \\cdot (F^i, F^{i+1}) = (N, F^{i+1})$ ，但是究竟为什么这样会有效呢？\n",
    "\n",
    "下面考虑一个非常简单的示例，图结构如下($N = 4$)，\n",
    "\n",
    "![GCN_01](../../../docs/Images/GCN_01.png)\n",
    "\n",
    "首先写出邻接矩阵 $A$ ($shape=(4, 4)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.matrix([\n",
    "        [0, 1, 0, 0],\n",
    "        [0, 0, 1, 1], \n",
    "        [0, 1, 0, 0],\n",
    "        [1, 0, 1, 0]\n",
    "    ],dtype=np.float64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后表示输入特征 $X$ ($shape=(4, 2)$) （这里输入特征维度为 $2$）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.,  0.],\n",
       "        [ 1., -1.],\n",
       "        [ 2., -2.],\n",
       "        [ 3., -3.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.matrix([\n",
    "        [i, -i] for i in range(A.shape[0])\n",
    "    ], dtype=np.float64)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "应用图卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1., -1.],\n",
       "        [ 5., -5.],\n",
       "        [ 1., -1.],\n",
       "        [ 2., -2.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以发现，原始特征在经过第一步图卷积之后在图上发生了传播，比如顶点 1 ，聚合了邻居 2 和 3 的特征\n",
    "\n",
    "> 如果存在从 $u$ 到 $v$ 的边，则 $v$ 是 $u$ 的邻居，也即有向图沿着箭头的反方向传播，无向图沿着边传播\n",
    "\n",
    "但是这样就会产生两个问题\n",
    "\n",
    "- 首先，每个顶点特征在传播后自身的信息会丢失，为了避免这一问题，可以通过**在图上增加自环**来解决，具体方法就是在 $A$ 的基础上增加单位矩阵 $I$ ，得到的修正结果为 $\\hat{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = np.matrix(np.eye(A.shape[0]))\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1., -1.],\n",
       "        [ 6., -6.],\n",
       "        [ 3., -3.],\n",
       "        [ 5., -5.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_hat = A + I\n",
    "A_hat * X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 另外还有一个问题就是，各个特征在聚合的过程中传播的次数取决于顶点度的大小，度大的顶点会使得特征的在整体表征所占权重更大，这可能会引发梯度消失或梯度爆炸，一种简单的想法就是**对度进行归一化**，至于归一化的方法，可以使用 $D^{-1} A$ ，论文中使用 $D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}$，道理都一样，本文暂时不涉及后者\n",
    "\n",
    "   > $D$ 为矩阵 $A$ 的度矩阵\n",
    "\n",
    "   首先写出获取一个矩阵的度矩阵的函数，并求得矩阵 $A$ 的度矩阵 $D$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 0., 0., 0.],\n",
       "        [0., 2., 0., 0.],\n",
       "        [0., 0., 2., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_degree_matrix(A):\n",
    "    D = np.array(np.sum(A, axis=0))[0]\n",
    "    D = np.matrix(np.diag(D))\n",
    "    return D\n",
    "D = get_degree_matrix(A)\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   下面求出归一化的邻接矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0. , 1. , 0. , 0. ],\n",
       "        [0. , 0. , 0.5, 0.5],\n",
       "        [0. , 0.5, 0. , 0. ],\n",
       "        [1. , 0. , 1. , 0. ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D**-1 * A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   很明显，邻接矩阵在度大的方向减小了链接权重（除以对应的度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1. , -1. ],\n",
       "        [ 2.5, -2.5],\n",
       "        [ 0.5, -0.5],\n",
       "        [ 2. , -2. ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D**-1 * A * X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   相应地，聚合时传播效果也是归一化的结果\n",
    "\n",
    "下面，将上述两个问题结合起来，首先求得 $\\hat{A}$ ，之后对 $\\hat{A}$ 进行归一化，当然，$\\hat{D}$ 是 $\\hat{A}$ 的度矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.5, -0.5],\n",
       "        [ 2. , -2. ],\n",
       "        [ 1. , -1. ],\n",
       "        [ 2.5, -2.5]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_hat = get_degree_matrix(A_hat)\n",
    "D_hat**-1 * A_hat * X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至于后面的参数，乘一下就好了，然后加上激活函数便完成一层图卷积网络层了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 0.],\n",
       "        [4., 0.],\n",
       "        [2., 0.],\n",
       "        [5., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.matrix([\n",
    "             [1, -1],\n",
    "             [-1, 1]\n",
    "         ])\n",
    "ReLU(D_hat**-1 * A_hat * X * W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 何谓卷积\n",
    "\n",
    "回顾传统卷积，可以看做是每个卷积核处，不同像素点的特征聚合于卷积核中心处而已\n",
    "\n",
    "![GCN_02](../../../docs/Images/GCN_02.png)\n",
    "\n",
    "而图卷积，是沿着图的边，将邻居的特征聚合于自身\n",
    "\n",
    "![GCN_03](../../../docs/Images/GCN_03.png)\n",
    "\n",
    "> 本图为无向图，若为有向图，聚合方向沿着箭头的反方向\n",
    "\n",
    "当然，前者发生在欧式空间，后者是在拓扑结构上，所谓卷积，可以说是相邻结点的一次信息聚合，而信息的聚合，[Ref3](https://www.zhihu.com/question/54504471)一文中首先使用了温度场模型的热量传播进行比拟，之后推到图模型，由浅及深地进行了解释\n",
    "\n",
    "对于一个连续的模型，$t$ 时刻的结果就是 $f(x, y, z, t)$ ，而一个离散的结构，每一时刻的结果都与前一时刻相关联，每一位置的结果都与周围位置相关联，在求得了前一时刻各位置的结果后，下一时刻任何一个位置都可以求得，每一个位置的结果取决于其相邻结点，具体关系可对原来的连续模型下的公式进行离散化，化微分为差分，便可得到相邻结点传播公式\n",
    "\n",
    "> 聚合针对某一结点，传播针对整个结构\n",
    "> 另外，由于本学期有一门专业课恰好学习并实践温度场模型的有限差分模拟，所以看到这篇文章倍感亲切~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 图卷积的优点\n",
    "\n",
    "图卷积是在传统网络的基础上增加了图的结构，增加的信息当然不是毫无用处，因为图的存在，使用随机初始化的参数便可完成初步的聚类，而且只需要较少的标签便可完成学习，因此 $GCN$ 也被称为是一种**半监督学习**方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "整个图卷积网络层，可以分为两步，第一步是图卷积，第二步是层与层之间的连接\n",
    "\n",
    "前者使用邻接矩阵 $A$ ，使得特征沿着图的边进行传播，得到 $A H^i$ ，考虑到自环问题和归一化问题的话，改为 $\\hat{D}^{-1} \\hat{A} H^i$ 或者 $D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}} \\hat{A} H^i$ 即可\n",
    "\n",
    "后者使用链接权重 $W^i$ ，与传统网络并无不同，其 $shape$ 依然为 $(f^{in}, f^{out})$ （这里 $in$ 和 $out$ 用来表示层的输入与输出），之后激活一下就好啦\n",
    "\n",
    "总的来说，图卷积不过是在前层特征计算完之后再整张图上传播一下（第一步），之后和传统网络并无区别，所以说， $GCN$ 也没啥难的嘛~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 下面是基于空手道俱乐部数据搭建的网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "zkc = networkx.karate_club_graph()\n",
    "order = sorted(list(zkc.nodes()))\n",
    "A = networkx.to_numpy_matrix(zkc, nodelist=order)\n",
    "I = np.eye(zkc.number_of_nodes())\n",
    "A_hat = A + I\n",
    "D_hat = np.array(np.sum(A_hat, axis=0))[0]\n",
    "D_hat = np.matrix(np.diag(D_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_1 = np.random.normal(\n",
    "    loc=0, scale=1, size=(zkc.number_of_nodes(), 4))\n",
    "W_2 = np.random.normal(\n",
    "    loc=0, size=(W_1.shape[1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_layer(A_hat, D_hat, X, W):\n",
    "    return ReLU(D_hat**-1 * A_hat * X * W)\n",
    "H_1 = gcn_layer(A_hat, D_hat, I, W_1)\n",
    "H_2 = gcn_layer(A_hat, D_hat, H_1, W_2)\n",
    "output = H_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_representations = {\n",
    "    node: np.array(output)[node] \n",
    "    for node in zkc.nodes()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "1. [图卷积网络到底怎么做，这是一份极简的 Numpy 实现](https://mp.weixin.qq.com/s/sg9O761F0KHAmCPOfMW_kQ)\n",
    "2. [何时能懂你的心——图卷积神经网络（GCN）](https://mp.weixin.qq.com/s/I3MsVSR0SNIKe-a9WRhGPQ)\n",
    "3. [如何理解 Graph Convolutional Network（GCN）？](https://www.zhihu.com/question/54504471)\n",
    "4. [GCN graph convolutional networks 详解](https://blog.csdn.net/guotong1988/article/details/82628156)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
